{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Gradient-free Global Maximum Likelihood Finding: Neural Networks\n",
    "\n",
    "Neural network training is typically done with maximum likelihood estimation. Given the number of parameter invariances, and symmetries in neural network architectures, this often introduces a large number of local minima, making global optimisation very difficult.\n",
    "\n",
    "JAXNS uses slice sampling as a gradient-free way to sample from hard-likelihood constraints, starting from small likelihoods and strictly increasing towards a maximum likelihood. This actually means that JAXNS is performing global maximisation of the likelihood. The prior can be seen as a measure which guides where JAXNS looks first. An attractive idea is to think about the prior as a guide for efficient global maximisation with JAXNS, but that's for another tutorial ;).\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this tutorial we'll cover:\n",
    "1. How to build a JAXNS model of a neural network using [Haiku](https://github.com/deepmind/dm-haiku)\n",
    "2. How to do global likelihood maximisation with JAXNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import haiku as hk\n",
    "except ImportError:\n",
    "    raise ImportError(\"You must `pip install dm-haiku` first.\")\n",
    "\n",
    "try:\n",
    "    from sklearn.metrics import roc_curve\n",
    "except:\n",
    "    raise ImportError(\"You must `pip install scikit-learn`\")\n",
    "\n",
    "from jax import numpy as jnp, random, vmap\n",
    "import jax\n",
    "from jax.flatten_util import ravel_pytree\n",
    "\n",
    "from jaxns.prior_transforms import UniformPrior, PriorChain\n",
    "from jaxns import GlobalOptimiser\n",
    "from jax.scipy.optimize import minimize\n",
    "from itertools import product\n",
    "import pylab as plt\n",
    "\n",
    "# for parallel sampling\n",
    "import os\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count=4\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO[2022-04-08 18:12:53,328]: Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: \n",
      "INFO[2022-04-08 18:12:53,329]: Unable to initialize backend 'gpu': NOT_FOUND: Could not find registered platform with name: \"cuda\". Available platform names are: Interpreter Host\n",
      "INFO[2022-04-08 18:12:53,329]: Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
      "WARNING[2022-04-08 18:12:53,330]: No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:\n",
      "[1. 1. 1. 1. 1. 1. 1. 1.] -> [False]\n",
      "[1. 1. 1. 1. 1. 1. 1. 0.] -> [ True]\n",
      "[1. 1. 1. 1. 1. 1. 0. 1.] -> [ True]\n",
      "[1. 1. 1. 1. 1. 1. 0. 0.] -> [False]\n",
      "[1. 1. 1. 1. 1. 0. 1. 1.] -> [ True]\n",
      "[1. 1. 1. 1. 1. 0. 1. 0.] -> [False]\n",
      "[1. 1. 1. 1. 1. 0. 0. 1.] -> [False]\n",
      "[1. 1. 1. 1. 1. 0. 0. 0.] -> [ True]\n",
      "[1. 1. 1. 1. 0. 1. 1. 1.] -> [ True]\n",
      "[1. 1. 1. 1. 0. 1. 1. 0.] -> [False]\n",
      "[1. 1. 1. 1. 0. 1. 0. 1.] -> [False]\n",
      "[1. 1. 1. 1. 0. 1. 0. 0.] -> [ True]\n",
      "[1. 1. 1. 1. 0. 0. 1. 1.] -> [False]\n",
      "[1. 1. 1. 1. 0. 0. 1. 0.] -> [ True]\n",
      "[1. 1. 1. 1. 0. 0. 0. 1.] -> [ True]\n",
      "[1. 1. 1. 1. 0. 0. 0. 0.] -> [False]\n",
      "[1. 1. 1. 0. 1. 1. 1. 1.] -> [ True]\n",
      "[1. 1. 1. 0. 1. 1. 1. 0.] -> [False]\n",
      "[1. 1. 1. 0. 1. 1. 0. 1.] -> [False]\n",
      "[1. 1. 1. 0. 1. 1. 0. 0.] -> [ True]\n",
      "[1. 1. 1. 0. 1. 0. 1. 1.] -> [False]\n",
      "[1. 1. 1. 0. 1. 0. 1. 0.] -> [ True]\n",
      "[1. 1. 1. 0. 1. 0. 0. 1.] -> [ True]\n",
      "[1. 1. 1. 0. 1. 0. 0. 0.] -> [False]\n",
      "[1. 1. 1. 0. 0. 1. 1. 1.] -> [False]\n",
      "[1. 1. 1. 0. 0. 1. 1. 0.] -> [ True]\n",
      "[1. 1. 1. 0. 0. 1. 0. 1.] -> [ True]\n",
      "[1. 1. 1. 0. 0. 1. 0. 0.] -> [False]\n",
      "[1. 1. 1. 0. 0. 0. 1. 1.] -> [ True]\n",
      "[1. 1. 1. 0. 0. 0. 1. 0.] -> [False]\n",
      "[1. 1. 1. 0. 0. 0. 0. 1.] -> [False]\n",
      "[1. 1. 1. 0. 0. 0. 0. 0.] -> [ True]\n",
      "[1. 1. 0. 1. 1. 1. 1. 1.] -> [ True]\n",
      "[1. 1. 0. 1. 1. 1. 1. 0.] -> [False]\n",
      "[1. 1. 0. 1. 1. 1. 0. 1.] -> [False]\n",
      "[1. 1. 0. 1. 1. 1. 0. 0.] -> [ True]\n",
      "[1. 1. 0. 1. 1. 0. 1. 1.] -> [False]\n",
      "[1. 1. 0. 1. 1. 0. 1. 0.] -> [ True]\n",
      "[1. 1. 0. 1. 1. 0. 0. 1.] -> [ True]\n",
      "[1. 1. 0. 1. 1. 0. 0. 0.] -> [False]\n",
      "[1. 1. 0. 1. 0. 1. 1. 1.] -> [False]\n",
      "[1. 1. 0. 1. 0. 1. 1. 0.] -> [ True]\n",
      "[1. 1. 0. 1. 0. 1. 0. 1.] -> [ True]\n",
      "[1. 1. 0. 1. 0. 1. 0. 0.] -> [False]\n",
      "[1. 1. 0. 1. 0. 0. 1. 1.] -> [ True]\n",
      "[1. 1. 0. 1. 0. 0. 1. 0.] -> [False]\n",
      "[1. 1. 0. 1. 0. 0. 0. 1.] -> [False]\n",
      "[1. 1. 0. 1. 0. 0. 0. 0.] -> [ True]\n",
      "[1. 1. 0. 0. 1. 1. 1. 1.] -> [False]\n",
      "[1. 1. 0. 0. 1. 1. 1. 0.] -> [ True]\n",
      "[1. 1. 0. 0. 1. 1. 0. 1.] -> [ True]\n",
      "[1. 1. 0. 0. 1. 1. 0. 0.] -> [False]\n",
      "[1. 1. 0. 0. 1. 0. 1. 1.] -> [ True]\n",
      "[1. 1. 0. 0. 1. 0. 1. 0.] -> [False]\n",
      "[1. 1. 0. 0. 1. 0. 0. 1.] -> [False]\n",
      "[1. 1. 0. 0. 1. 0. 0. 0.] -> [ True]\n",
      "[1. 1. 0. 0. 0. 1. 1. 1.] -> [ True]\n",
      "[1. 1. 0. 0. 0. 1. 1. 0.] -> [False]\n",
      "[1. 1. 0. 0. 0. 1. 0. 1.] -> [False]\n",
      "[1. 1. 0. 0. 0. 1. 0. 0.] -> [ True]\n",
      "[1. 1. 0. 0. 0. 0. 1. 1.] -> [False]\n",
      "[1. 1. 0. 0. 0. 0. 1. 0.] -> [ True]\n",
      "[1. 1. 0. 0. 0. 0. 0. 1.] -> [ True]\n",
      "[1. 1. 0. 0. 0. 0. 0. 0.] -> [False]\n",
      "[1. 0. 1. 1. 1. 1. 1. 1.] -> [ True]\n",
      "[1. 0. 1. 1. 1. 1. 1. 0.] -> [False]\n",
      "[1. 0. 1. 1. 1. 1. 0. 1.] -> [False]\n",
      "[1. 0. 1. 1. 1. 1. 0. 0.] -> [ True]\n",
      "[1. 0. 1. 1. 1. 0. 1. 1.] -> [False]\n",
      "[1. 0. 1. 1. 1. 0. 1. 0.] -> [ True]\n",
      "[1. 0. 1. 1. 1. 0. 0. 1.] -> [ True]\n",
      "[1. 0. 1. 1. 1. 0. 0. 0.] -> [False]\n",
      "[1. 0. 1. 1. 0. 1. 1. 1.] -> [False]\n",
      "[1. 0. 1. 1. 0. 1. 1. 0.] -> [ True]\n",
      "[1. 0. 1. 1. 0. 1. 0. 1.] -> [ True]\n",
      "[1. 0. 1. 1. 0. 1. 0. 0.] -> [False]\n",
      "[1. 0. 1. 1. 0. 0. 1. 1.] -> [ True]\n",
      "[1. 0. 1. 1. 0. 0. 1. 0.] -> [False]\n",
      "[1. 0. 1. 1. 0. 0. 0. 1.] -> [False]\n",
      "[1. 0. 1. 1. 0. 0. 0. 0.] -> [ True]\n",
      "[1. 0. 1. 0. 1. 1. 1. 1.] -> [False]\n",
      "[1. 0. 1. 0. 1. 1. 1. 0.] -> [ True]\n",
      "[1. 0. 1. 0. 1. 1. 0. 1.] -> [ True]\n",
      "[1. 0. 1. 0. 1. 1. 0. 0.] -> [False]\n",
      "[1. 0. 1. 0. 1. 0. 1. 1.] -> [ True]\n",
      "[1. 0. 1. 0. 1. 0. 1. 0.] -> [False]\n",
      "[1. 0. 1. 0. 1. 0. 0. 1.] -> [False]\n",
      "[1. 0. 1. 0. 1. 0. 0. 0.] -> [ True]\n",
      "[1. 0. 1. 0. 0. 1. 1. 1.] -> [ True]\n",
      "[1. 0. 1. 0. 0. 1. 1. 0.] -> [False]\n",
      "[1. 0. 1. 0. 0. 1. 0. 1.] -> [False]\n",
      "[1. 0. 1. 0. 0. 1. 0. 0.] -> [ True]\n",
      "[1. 0. 1. 0. 0. 0. 1. 1.] -> [False]\n",
      "[1. 0. 1. 0. 0. 0. 1. 0.] -> [ True]\n",
      "[1. 0. 1. 0. 0. 0. 0. 1.] -> [ True]\n",
      "[1. 0. 1. 0. 0. 0. 0. 0.] -> [False]\n",
      "[1. 0. 0. 1. 1. 1. 1. 1.] -> [False]\n",
      "[1. 0. 0. 1. 1. 1. 1. 0.] -> [ True]\n",
      "[1. 0. 0. 1. 1. 1. 0. 1.] -> [ True]\n",
      "[1. 0. 0. 1. 1. 1. 0. 0.] -> [False]\n",
      "[1. 0. 0. 1. 1. 0. 1. 1.] -> [ True]\n",
      "[1. 0. 0. 1. 1. 0. 1. 0.] -> [False]\n",
      "[1. 0. 0. 1. 1. 0. 0. 1.] -> [False]\n",
      "[1. 0. 0. 1. 1. 0. 0. 0.] -> [ True]\n",
      "[1. 0. 0. 1. 0. 1. 1. 1.] -> [ True]\n",
      "[1. 0. 0. 1. 0. 1. 1. 0.] -> [False]\n",
      "[1. 0. 0. 1. 0. 1. 0. 1.] -> [False]\n",
      "[1. 0. 0. 1. 0. 1. 0. 0.] -> [ True]\n",
      "[1. 0. 0. 1. 0. 0. 1. 1.] -> [False]\n",
      "[1. 0. 0. 1. 0. 0. 1. 0.] -> [ True]\n",
      "[1. 0. 0. 1. 0. 0. 0. 1.] -> [ True]\n",
      "[1. 0. 0. 1. 0. 0. 0. 0.] -> [False]\n",
      "[1. 0. 0. 0. 1. 1. 1. 1.] -> [ True]\n",
      "[1. 0. 0. 0. 1. 1. 1. 0.] -> [False]\n",
      "[1. 0. 0. 0. 1. 1. 0. 1.] -> [False]\n",
      "[1. 0. 0. 0. 1. 1. 0. 0.] -> [ True]\n",
      "[1. 0. 0. 0. 1. 0. 1. 1.] -> [False]\n",
      "[1. 0. 0. 0. 1. 0. 1. 0.] -> [ True]\n",
      "[1. 0. 0. 0. 1. 0. 0. 1.] -> [ True]\n",
      "[1. 0. 0. 0. 1. 0. 0. 0.] -> [False]\n",
      "[1. 0. 0. 0. 0. 1. 1. 1.] -> [False]\n",
      "[1. 0. 0. 0. 0. 1. 1. 0.] -> [ True]\n",
      "[1. 0. 0. 0. 0. 1. 0. 1.] -> [ True]\n",
      "[1. 0. 0. 0. 0. 1. 0. 0.] -> [False]\n",
      "[1. 0. 0. 0. 0. 0. 1. 1.] -> [ True]\n",
      "[1. 0. 0. 0. 0. 0. 1. 0.] -> [False]\n",
      "[1. 0. 0. 0. 0. 0. 0. 1.] -> [False]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0.] -> [ True]\n",
      "[0. 1. 1. 1. 1. 1. 1. 1.] -> [ True]\n",
      "[0. 1. 1. 1. 1. 1. 1. 0.] -> [False]\n",
      "[0. 1. 1. 1. 1. 1. 0. 1.] -> [False]\n",
      "[0. 1. 1. 1. 1. 1. 0. 0.] -> [ True]\n",
      "[0. 1. 1. 1. 1. 0. 1. 1.] -> [False]\n",
      "[0. 1. 1. 1. 1. 0. 1. 0.] -> [ True]\n",
      "[0. 1. 1. 1. 1. 0. 0. 1.] -> [ True]\n",
      "[0. 1. 1. 1. 1. 0. 0. 0.] -> [False]\n",
      "[0. 1. 1. 1. 0. 1. 1. 1.] -> [False]\n",
      "[0. 1. 1. 1. 0. 1. 1. 0.] -> [ True]\n",
      "[0. 1. 1. 1. 0. 1. 0. 1.] -> [ True]\n",
      "[0. 1. 1. 1. 0. 1. 0. 0.] -> [False]\n",
      "[0. 1. 1. 1. 0. 0. 1. 1.] -> [ True]\n",
      "[0. 1. 1. 1. 0. 0. 1. 0.] -> [False]\n",
      "[0. 1. 1. 1. 0. 0. 0. 1.] -> [False]\n",
      "[0. 1. 1. 1. 0. 0. 0. 0.] -> [ True]\n",
      "[0. 1. 1. 0. 1. 1. 1. 1.] -> [False]\n",
      "[0. 1. 1. 0. 1. 1. 1. 0.] -> [ True]\n",
      "[0. 1. 1. 0. 1. 1. 0. 1.] -> [ True]\n",
      "[0. 1. 1. 0. 1. 1. 0. 0.] -> [False]\n",
      "[0. 1. 1. 0. 1. 0. 1. 1.] -> [ True]\n",
      "[0. 1. 1. 0. 1. 0. 1. 0.] -> [False]\n",
      "[0. 1. 1. 0. 1. 0. 0. 1.] -> [False]\n",
      "[0. 1. 1. 0. 1. 0. 0. 0.] -> [ True]\n",
      "[0. 1. 1. 0. 0. 1. 1. 1.] -> [ True]\n",
      "[0. 1. 1. 0. 0. 1. 1. 0.] -> [False]\n",
      "[0. 1. 1. 0. 0. 1. 0. 1.] -> [False]\n",
      "[0. 1. 1. 0. 0. 1. 0. 0.] -> [ True]\n",
      "[0. 1. 1. 0. 0. 0. 1. 1.] -> [False]\n",
      "[0. 1. 1. 0. 0. 0. 1. 0.] -> [ True]\n",
      "[0. 1. 1. 0. 0. 0. 0. 1.] -> [ True]\n",
      "[0. 1. 1. 0. 0. 0. 0. 0.] -> [False]\n",
      "[0. 1. 0. 1. 1. 1. 1. 1.] -> [False]\n",
      "[0. 1. 0. 1. 1. 1. 1. 0.] -> [ True]\n",
      "[0. 1. 0. 1. 1. 1. 0. 1.] -> [ True]\n",
      "[0. 1. 0. 1. 1. 1. 0. 0.] -> [False]\n",
      "[0. 1. 0. 1. 1. 0. 1. 1.] -> [ True]\n",
      "[0. 1. 0. 1. 1. 0. 1. 0.] -> [False]\n",
      "[0. 1. 0. 1. 1. 0. 0. 1.] -> [False]\n",
      "[0. 1. 0. 1. 1. 0. 0. 0.] -> [ True]\n",
      "[0. 1. 0. 1. 0. 1. 1. 1.] -> [ True]\n",
      "[0. 1. 0. 1. 0. 1. 1. 0.] -> [False]\n",
      "[0. 1. 0. 1. 0. 1. 0. 1.] -> [False]\n",
      "[0. 1. 0. 1. 0. 1. 0. 0.] -> [ True]\n",
      "[0. 1. 0. 1. 0. 0. 1. 1.] -> [False]\n",
      "[0. 1. 0. 1. 0. 0. 1. 0.] -> [ True]\n",
      "[0. 1. 0. 1. 0. 0. 0. 1.] -> [ True]\n",
      "[0. 1. 0. 1. 0. 0. 0. 0.] -> [False]\n",
      "[0. 1. 0. 0. 1. 1. 1. 1.] -> [ True]\n",
      "[0. 1. 0. 0. 1. 1. 1. 0.] -> [False]\n",
      "[0. 1. 0. 0. 1. 1. 0. 1.] -> [False]\n",
      "[0. 1. 0. 0. 1. 1. 0. 0.] -> [ True]\n",
      "[0. 1. 0. 0. 1. 0. 1. 1.] -> [False]\n",
      "[0. 1. 0. 0. 1. 0. 1. 0.] -> [ True]\n",
      "[0. 1. 0. 0. 1. 0. 0. 1.] -> [ True]\n",
      "[0. 1. 0. 0. 1. 0. 0. 0.] -> [False]\n",
      "[0. 1. 0. 0. 0. 1. 1. 1.] -> [False]\n",
      "[0. 1. 0. 0. 0. 1. 1. 0.] -> [ True]\n",
      "[0. 1. 0. 0. 0. 1. 0. 1.] -> [ True]\n",
      "[0. 1. 0. 0. 0. 1. 0. 0.] -> [False]\n",
      "[0. 1. 0. 0. 0. 0. 1. 1.] -> [ True]\n",
      "[0. 1. 0. 0. 0. 0. 1. 0.] -> [False]\n",
      "[0. 1. 0. 0. 0. 0. 0. 1.] -> [False]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0.] -> [ True]\n",
      "[0. 0. 1. 1. 1. 1. 1. 1.] -> [False]\n",
      "[0. 0. 1. 1. 1. 1. 1. 0.] -> [ True]\n",
      "[0. 0. 1. 1. 1. 1. 0. 1.] -> [ True]\n",
      "[0. 0. 1. 1. 1. 1. 0. 0.] -> [False]\n",
      "[0. 0. 1. 1. 1. 0. 1. 1.] -> [ True]\n",
      "[0. 0. 1. 1. 1. 0. 1. 0.] -> [False]\n",
      "[0. 0. 1. 1. 1. 0. 0. 1.] -> [False]\n",
      "[0. 0. 1. 1. 1. 0. 0. 0.] -> [ True]\n",
      "[0. 0. 1. 1. 0. 1. 1. 1.] -> [ True]\n",
      "[0. 0. 1. 1. 0. 1. 1. 0.] -> [False]\n",
      "[0. 0. 1. 1. 0. 1. 0. 1.] -> [False]\n",
      "[0. 0. 1. 1. 0. 1. 0. 0.] -> [ True]\n",
      "[0. 0. 1. 1. 0. 0. 1. 1.] -> [False]\n",
      "[0. 0. 1. 1. 0. 0. 1. 0.] -> [ True]\n",
      "[0. 0. 1. 1. 0. 0. 0. 1.] -> [ True]\n",
      "[0. 0. 1. 1. 0. 0. 0. 0.] -> [False]\n",
      "[0. 0. 1. 0. 1. 1. 1. 1.] -> [ True]\n",
      "[0. 0. 1. 0. 1. 1. 1. 0.] -> [False]\n",
      "[0. 0. 1. 0. 1. 1. 0. 1.] -> [False]\n",
      "[0. 0. 1. 0. 1. 1. 0. 0.] -> [ True]\n",
      "[0. 0. 1. 0. 1. 0. 1. 1.] -> [False]\n",
      "[0. 0. 1. 0. 1. 0. 1. 0.] -> [ True]\n",
      "[0. 0. 1. 0. 1. 0. 0. 1.] -> [ True]\n",
      "[0. 0. 1. 0. 1. 0. 0. 0.] -> [False]\n",
      "[0. 0. 1. 0. 0. 1. 1. 1.] -> [False]\n",
      "[0. 0. 1. 0. 0. 1. 1. 0.] -> [ True]\n",
      "[0. 0. 1. 0. 0. 1. 0. 1.] -> [ True]\n",
      "[0. 0. 1. 0. 0. 1. 0. 0.] -> [False]\n",
      "[0. 0. 1. 0. 0. 0. 1. 1.] -> [ True]\n",
      "[0. 0. 1. 0. 0. 0. 1. 0.] -> [False]\n",
      "[0. 0. 1. 0. 0. 0. 0. 1.] -> [False]\n",
      "[0. 0. 1. 0. 0. 0. 0. 0.] -> [ True]\n",
      "[0. 0. 0. 1. 1. 1. 1. 1.] -> [ True]\n",
      "[0. 0. 0. 1. 1. 1. 1. 0.] -> [False]\n",
      "[0. 0. 0. 1. 1. 1. 0. 1.] -> [False]\n",
      "[0. 0. 0. 1. 1. 1. 0. 0.] -> [ True]\n",
      "[0. 0. 0. 1. 1. 0. 1. 1.] -> [False]\n",
      "[0. 0. 0. 1. 1. 0. 1. 0.] -> [ True]\n",
      "[0. 0. 0. 1. 1. 0. 0. 1.] -> [ True]\n",
      "[0. 0. 0. 1. 1. 0. 0. 0.] -> [False]\n",
      "[0. 0. 0. 1. 0. 1. 1. 1.] -> [False]\n",
      "[0. 0. 0. 1. 0. 1. 1. 0.] -> [ True]\n",
      "[0. 0. 0. 1. 0. 1. 0. 1.] -> [ True]\n",
      "[0. 0. 0. 1. 0. 1. 0. 0.] -> [False]\n",
      "[0. 0. 0. 1. 0. 0. 1. 1.] -> [ True]\n",
      "[0. 0. 0. 1. 0. 0. 1. 0.] -> [False]\n",
      "[0. 0. 0. 1. 0. 0. 0. 1.] -> [False]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0.] -> [ True]\n",
      "[0. 0. 0. 0. 1. 1. 1. 1.] -> [False]\n",
      "[0. 0. 0. 0. 1. 1. 1. 0.] -> [ True]\n",
      "[0. 0. 0. 0. 1. 1. 0. 1.] -> [ True]\n",
      "[0. 0. 0. 0. 1. 1. 0. 0.] -> [False]\n",
      "[0. 0. 0. 0. 1. 0. 1. 1.] -> [ True]\n",
      "[0. 0. 0. 0. 1. 0. 1. 0.] -> [False]\n",
      "[0. 0. 0. 0. 1. 0. 0. 1.] -> [False]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0.] -> [ True]\n",
      "[0. 0. 0. 0. 0. 1. 1. 1.] -> [ True]\n",
      "[0. 0. 0. 0. 0. 1. 1. 0.] -> [False]\n",
      "[0. 0. 0. 0. 0. 1. 0. 1.] -> [False]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0.] -> [ True]\n",
      "[0. 0. 0. 0. 0. 0. 1. 1.] -> [False]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0.] -> [ True]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1.] -> [ True]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0.] -> [False]\n"
     ]
    }
   ],
   "source": [
    "# Generate data\n",
    "\n",
    "def xor_reduce(x):\n",
    "    \"\"\"\n",
    "    Computes the XOR reduction on a sequence of bits.\n",
    "\n",
    "    Examples:\n",
    "        100 -> xor(xor(1,0),0) = 1\n",
    "        001 -> xor(xor(0,0),1) = 1\n",
    "        110 -> xor(xor(1,1),0) = 0\n",
    "        011 -> xor(xor(0,1),1) = 0\n",
    "\n",
    "    Args:\n",
    "        x: boolean vector of bits.\n",
    "\n",
    "    Returns:\n",
    "        bool, scalar\n",
    "    \"\"\"\n",
    "    output = x[0]\n",
    "    for i in range(1, x.shape[-1]):\n",
    "        output = jnp.logical_xor(output, x[i])\n",
    "    return output\n",
    "\n",
    "\n",
    "num_variables = 8\n",
    "options = [True, False]\n",
    "x = jnp.asarray(list(product(options, repeat=num_variables)))#N,2\n",
    "y = vmap(xor_reduce)(x)[:, None]#N, 1\n",
    "x = x.astype(jnp.float32)\n",
    "print(\"Data:\")\n",
    "\n",
    "for input, output in zip(x,y):\n",
    "    print(f\"{input} -> {output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 31\n"
     ]
    }
   ],
   "source": [
    "# Define the likelihood, using Haiku as our framework for neural networks\n",
    "\n",
    "def model(x, is_training=False):\n",
    "    mlp1 = hk.Sequential([hk.Linear(3),\n",
    "                         jax.nn.sigmoid,\n",
    "                          hk.Linear(1)])\n",
    "\n",
    "    return mlp1(x)\n",
    "\n",
    "model = hk.without_apply_rng(hk.transform(model))\n",
    "# We must call the model once to get the params shape and type as a big pytree\n",
    "# We then use ravel_pytree to flatten and get the unflatten function.\n",
    "init_params = model.init(random.PRNGKey(2345), x)\n",
    "init_params_flat, unravel_func = ravel_pytree(init_params)\n",
    "n_dims = init_params_flat.size\n",
    "print(\"Number of parameters:\", n_dims)\n",
    "\n",
    "def softplus(x):\n",
    "    return jnp.log1p(jnp.exp(x))\n",
    "\n",
    "def log_likelihood(params):\n",
    "    \"\"\"\n",
    "    log(P(y|p))\n",
    "    p = exp(logits)/1 - exp(logits)\n",
    "    = log(p) * y + log(1-p) * (1-y)\n",
    "    = logits * y1 - log(exp(-logits)/(exp(-logits) - 1)) * y0\n",
    "    \"\"\"\n",
    "    params_dict = unravel_func(params)\n",
    "    logits = model.apply(params_dict, x)\n",
    "    log_prob0, log_prob1 = -softplus(logits), -softplus(-logits)\n",
    "    #log(p) * y + log(1-p) * (1-y)\n",
    "    log_prob = jnp.mean(jnp.where(y, log_prob1, log_prob0))\n",
    "    return jnp.asarray(log_prob, jnp.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/albert/miniconda3/envs/jax_py/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py:2565: UserWarning: Explicitly requested dtype <class 'jax._src.numpy.lax_numpy.float64'> requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  lax_internal._check_user_dtype_supported(dtype, \"asarray\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BFGS maximum likelihood solution of 100 tries: log(L) = -0.30103886127471924\n"
     ]
    }
   ],
   "source": [
    "# Let us compare the results of nested sampling to optimisation done with BFGS\n",
    "num_random_init = 100\n",
    "init_keys = random.split(random.PRNGKey(42), num_random_init)\n",
    "params_bfgs = vmap(lambda key: minimize(lambda p: -log_likelihood(p),\n",
    "                   random.normal(key, shape=(n_dims,)),\n",
    "                   method='BFGS').x)(init_keys)\n",
    "log_L_bfgs = vmap(log_likelihood)(params_bfgs)\n",
    "idx_max = jnp.argmax(log_L_bfgs)\n",
    "log_L_bfgs_max = log_L_bfgs[idx_max]\n",
    "params_bfgs_max = params_bfgs[idx_max]\n",
    "print(f\"BFGS maximum likelihood solution of {num_random_init} tries: log(L) = {log_L_bfgs_max}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Build the model\n",
    "\n",
    "with PriorChain() as prior_chain:\n",
    "    # we'll effectively place no prior on the parameters, other than requiring them to be within [-10,10]\n",
    "    UniformPrior('params', -10.*jnp.ones(n_dims), 10.*jnp.ones(n_dims))\n",
    "\n",
    "# We'll do some strange things here.\n",
    "# num_slices -> low: We'll make the sampler do very few slices. This will lead to large correlation between samples, and poor estimate of the evidence.\n",
    "# This is alright, because we'll be looking for the maximum likelihood solution.\n",
    "go = GlobalOptimiser(loglikelihood=log_likelihood, prior_chain=prior_chain,\n",
    "                   samples_per_step=prior_chain.U_ndims*10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/albert/miniconda3/envs/jax_py/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py:2565: UserWarning: Explicitly requested dtype <class 'jax._src.numpy.lax_numpy.float64'> requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  lax_internal._check_user_dtype_supported(dtype, \"asarray\")\n",
      "INFO[2022-04-08 18:13:12,310]: Log-likelihood: -8.569480895996094\n",
      "/home/albert/miniconda3/envs/jax_py/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py:2565: UserWarning: Explicitly requested dtype <class 'jax._src.numpy.lax_numpy.float64'> requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  lax_internal._check_user_dtype_supported(dtype, \"asarray\")\n",
      "INFO[2022-04-08 18:13:12,319]: Log-likelihood: -3.1986632347106934\n",
      "/home/albert/miniconda3/envs/jax_py/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py:2565: UserWarning: Explicitly requested dtype <class 'jax._src.numpy.lax_numpy.float64'> requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  lax_internal._check_user_dtype_supported(dtype, \"asarray\")\n",
      "INFO[2022-04-08 18:13:12,327]: Log-likelihood: -3.4121766090393066\n",
      "/home/albert/miniconda3/envs/jax_py/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py:2565: UserWarning: Explicitly requested dtype <class 'jax._src.numpy.lax_numpy.float64'> requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  lax_internal._check_user_dtype_supported(dtype, \"asarray\")\n",
      "INFO[2022-04-08 18:13:12,335]: Log-likelihood: -2.16805362701416\n",
      "/home/albert/miniconda3/envs/jax_py/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py:2565: UserWarning: Explicitly requested dtype <class 'jax._src.numpy.lax_numpy.float64'> requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  lax_internal._check_user_dtype_supported(dtype, \"asarray\")\n",
      "INFO[2022-04-08 18:13:12,344]: Log-likelihood: -3.359581470489502\n",
      "/home/albert/miniconda3/envs/jax_py/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py:2565: UserWarning: Explicitly requested dtype <class 'jax._src.numpy.lax_numpy.float64'> requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  lax_internal._check_user_dtype_supported(dtype, \"asarray\")\n",
      "INFO[2022-04-08 18:13:12,352]: Log-likelihood: -2.2838666439056396\n",
      "/home/albert/miniconda3/envs/jax_py/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py:2565: UserWarning: Explicitly requested dtype <class 'jax._src.numpy.lax_numpy.float64'> requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  lax_internal._check_user_dtype_supported(dtype, \"asarray\")\n",
      "INFO[2022-04-08 18:13:12,361]: Log-likelihood: -2.4835166931152344\n",
      "/home/albert/miniconda3/envs/jax_py/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py:2565: UserWarning: Explicitly requested dtype <class 'jax._src.numpy.lax_numpy.float64'> requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  lax_internal._check_user_dtype_supported(dtype, \"asarray\")\n",
      "INFO[2022-04-08 18:13:12,369]: Log-likelihood: -2.0809831619262695\n",
      "/home/albert/miniconda3/envs/jax_py/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py:2565: UserWarning: Explicitly requested dtype <class 'jax._src.numpy.lax_numpy.float64'> requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  lax_internal._check_user_dtype_supported(dtype, \"asarray\")\n",
      "INFO[2022-04-08 18:13:12,377]: Log-likelihood: -1.7735052108764648\n",
      "/home/albert/miniconda3/envs/jax_py/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py:2565: UserWarning: Explicitly requested dtype <class 'jax._src.numpy.lax_numpy.float64'> requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  lax_internal._check_user_dtype_supported(dtype, \"asarray\")\n",
      "INFO[2022-04-08 18:13:12,389]: Log-likelihood: -0.7664767503738403\n"
     ]
    }
   ],
   "source": [
    "# Let's test the model with a small sanity check.\n",
    "prior_chain.test_prior(random.PRNGKey(42), 10, log_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/albert/miniconda3/envs/jax_py/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py:2565: UserWarning: Explicitly requested dtype <class 'jax._src.numpy.lax_numpy.float64'> requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  lax_internal._check_user_dtype_supported(dtype, \"asarray\")\n"
     ]
    }
   ],
   "source": [
    "results, state = go(random.PRNGKey(42),\n",
    "             termination_frac_likelihood_improvement=1e-3,\n",
    "             termination_patience=3,\n",
    "             termination_max_num_likelihood_evaluations=20e6, return_state=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log L_max(L) -0.3232653\n"
     ]
    }
   ],
   "source": [
    "# The maximum likelihood solution from nested sampling\n",
    "params_max = results.sample_L_max['params']\n",
    "log_L_max = results.log_L_max\n",
    "print(\"log L_max(L)\", log_L_max)\n",
    "go.summary(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "source": [
    "# Let's try to refine this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, state = go(random.PRNGKey(42),\n",
    "             termination_frac_likelihood_improvement=1e-3,\n",
    "             termination_patience=3,\n",
    "             termination_max_num_likelihood_evaluations=20e6,\n",
    "             refine_state=state, return_state=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log L_max(L) -0.30872047\n",
      "--------\n",
      "Termination Conditions:\n",
      "Small enough likelihood change with patience\n",
      "--------\n",
      "# likelihood evals: 15391083\n",
      "# samples: 21993\n",
      "# likelihood evals / sample: 699.8\n",
      "--------\n",
      "Maximum logL=-0.31\n",
      "--------\n",
      "params[#]: max(L) est.\n",
      "params[0]: 4.1\n",
      "params[1]: 2.9\n",
      "params[2]: 9.5\n",
      "params[3]: 9.72\n",
      "params[4]: -8.58\n",
      "params[5]: 9.23\n",
      "params[6]: 8.2\n",
      "params[7]: -9.5\n",
      "params[8]: 9.7\n",
      "params[9]: 9.7\n",
      "params[10]: -9.9\n",
      "params[11]: 9.2\n",
      "params[12]: -9.5\n",
      "params[13]: 9.6\n",
      "params[14]: -8.5\n",
      "params[15]: -9.5\n",
      "params[16]: 10.0\n",
      "params[17]: -7.3\n",
      "params[18]: -8.3\n",
      "params[19]: 9.3\n",
      "params[20]: -8.7\n",
      "params[21]: 9.52\n",
      "params[22]: -9.4\n",
      "params[23]: 9.13\n",
      "params[24]: -8.98\n",
      "params[25]: 9.46\n",
      "params[26]: -8.57\n",
      "params[27]: 4.4\n",
      "params[28]: -9.9\n",
      "params[29]: -5.7\n",
      "params[30]: 6.4\n",
      "--------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'--------\\nTermination Conditions:\\nSmall enough likelihood change with patience\\n--------\\n# likelihood evals: 15391083\\n# samples: 21993\\n# likelihood evals / sample: 699.8\\n--------\\nMaximum logL=-0.31\\n--------\\nparams[#]: max(L) est.\\nparams[0]: 4.1\\nparams[1]: 2.9\\nparams[2]: 9.5\\nparams[3]: 9.72\\nparams[4]: -8.58\\nparams[5]: 9.23\\nparams[6]: 8.2\\nparams[7]: -9.5\\nparams[8]: 9.7\\nparams[9]: 9.7\\nparams[10]: -9.9\\nparams[11]: 9.2\\nparams[12]: -9.5\\nparams[13]: 9.6\\nparams[14]: -8.5\\nparams[15]: -9.5\\nparams[16]: 10.0\\nparams[17]: -7.3\\nparams[18]: -8.3\\nparams[19]: 9.3\\nparams[20]: -8.7\\nparams[21]: 9.52\\nparams[22]: -9.4\\nparams[23]: 9.13\\nparams[24]: -8.98\\nparams[25]: 9.46\\nparams[26]: -8.57\\nparams[27]: 4.4\\nparams[28]: -9.9\\nparams[29]: -5.7\\nparams[30]: 6.4\\n--------'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The maximum likelihood solution from nested sampling\n",
    "params_max = results.sample_L_max['params']\n",
    "log_L_max = results.log_L_max\n",
    "print(\"log L_max(L)\", log_L_max)\n",
    "go.summary(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def predict(params):\n",
    "    params_dict = unravel_func(params)\n",
    "    logits = model.apply(params_dict, x)\n",
    "    return jax.nn.sigmoid(logits)[:,0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions of globally optimised NN:\n",
      "0: [1. 1. 1. 1. 1. 1. 1. 1.] -> [False] | pred: 0.027095776051282883\n",
      "1: [1. 1. 1. 1. 1. 1. 1. 0.] -> [ True] | pred: 0.6903703808784485\n",
      "2: [1. 1. 1. 1. 1. 1. 0. 1.] -> [ True] | pred: 0.9817206859588623\n",
      "3: [1. 1. 1. 1. 1. 1. 0. 0.] -> [False] | pred: 0.04675538092851639\n",
      "4: [1. 1. 1. 1. 1. 0. 1. 1.] -> [ True] | pred: 0.6910300254821777\n",
      "5: [1. 1. 1. 1. 1. 0. 1. 0.] -> [False] | pred: 0.6916893720626831\n",
      "6: [1. 1. 1. 1. 1. 0. 0. 1.] -> [False] | pred: 0.09856195747852325\n",
      "7: [1. 1. 1. 1. 1. 0. 0. 0.] -> [ True] | pred: 0.6913754940032959\n",
      "8: [1. 1. 1. 1. 0. 1. 1. 1.] -> [ True] | pred: 0.6905560493469238\n",
      "9: [1. 1. 1. 1. 0. 1. 1. 0.] -> [False] | pred: 0.6916893720626831\n",
      "10: [1. 1. 1. 1. 0. 1. 0. 1.] -> [False] | pred: 0.05862881988286972\n",
      "11: [1. 1. 1. 1. 0. 1. 0. 0.] -> [ True] | pred: 0.691146731376648\n",
      "12: [1. 1. 1. 1. 0. 0. 1. 1.] -> [False] | pred: 0.6916893720626831\n",
      "13: [1. 1. 1. 1. 0. 0. 1. 0.] -> [ True] | pred: 0.6916896104812622\n",
      "14: [1. 1. 1. 1. 0. 0. 0. 1.] -> [ True] | pred: 0.6914190053939819\n",
      "15: [1. 1. 1. 1. 0. 0. 0. 0.] -> [False] | pred: 0.6916896104812622\n",
      "16: [1. 1. 1. 0. 1. 1. 1. 1.] -> [ True] | pred: 0.6898898482322693\n",
      "17: [1. 1. 1. 0. 1. 1. 1. 0.] -> [False] | pred: 0.691689133644104\n",
      "18: [1. 1. 1. 0. 1. 1. 0. 1.] -> [False] | pred: 0.03552545607089996\n",
      "19: [1. 1. 1. 0. 1. 1. 0. 0.] -> [ True] | pred: 0.6908329129219055\n",
      "20: [1. 1. 1. 0. 1. 0. 1. 1.] -> [False] | pred: 0.6916893720626831\n",
      "21: [1. 1. 1. 0. 1. 0. 1. 0.] -> [ True] | pred: 0.6916896104812622\n",
      "22: [1. 1. 1. 0. 1. 0. 0. 1.] -> [ True] | pred: 0.6912609338760376\n",
      "23: [1. 1. 1. 0. 1. 0. 0. 0.] -> [False] | pred: 0.6916896104812622\n",
      "24: [1. 1. 1. 0. 0. 1. 1. 1.] -> [False] | pred: 0.691689133644104\n",
      "25: [1. 1. 1. 0. 0. 1. 1. 0.] -> [ True] | pred: 0.6916896104812622\n",
      "26: [1. 1. 1. 0. 0. 1. 0. 1.] -> [ True] | pred: 0.690949022769928\n",
      "27: [1. 1. 1. 0. 0. 1. 0. 0.] -> [False] | pred: 0.6916893720626831\n",
      "28: [1. 1. 1. 0. 0. 0. 1. 1.] -> [ True] | pred: 0.6916896104812622\n",
      "29: [1. 1. 1. 0. 0. 0. 1. 0.] -> [False] | pred: 0.6916896104812622\n",
      "30: [1. 1. 1. 0. 0. 0. 0. 1.] -> [False] | pred: 0.6916896104812622\n",
      "31: [1. 1. 1. 0. 0. 0. 0. 0.] -> [ True] | pred: 0.6916896104812622\n",
      "32: [1. 1. 0. 1. 1. 1. 1. 1.] -> [ True] | pred: 0.9905863404273987\n",
      "33: [1. 1. 0. 1. 1. 1. 1. 0.] -> [False] | pred: 0.022095005959272385\n",
      "34: [1. 1. 0. 1. 1. 1. 0. 1.] -> [False] | pred: 0.32011571526527405\n",
      "35: [1. 1. 0. 1. 1. 1. 0. 0.] -> [ True] | pred: 0.9856378436088562\n",
      "36: [1. 1. 0. 1. 1. 0. 1. 1.] -> [False] | pred: 0.03406612575054169\n",
      "37: [1. 1. 0. 1. 1. 0. 1. 0.] -> [ True] | pred: 0.6906899213790894\n",
      "38: [1. 1. 0. 1. 1. 0. 0. 1.] -> [ True] | pred: 0.9866851568222046\n",
      "39: [1. 1. 0. 1. 1. 0. 0. 0.] -> [False] | pred: 0.06231983006000519\n",
      "40: [1. 1. 0. 1. 0. 1. 1. 1.] -> [False] | pred: 0.04734870418906212\n",
      "41: [1. 1. 0. 1. 0. 1. 1. 0.] -> [ True] | pred: 0.6899704337120056\n",
      "42: [1. 1. 0. 1. 0. 1. 0. 1.] -> [ True] | pred: 0.9864510297775269\n",
      "43: [1. 1. 0. 1. 0. 1. 0. 0.] -> [False] | pred: 0.0412835031747818\n",
      "44: [1. 1. 0. 1. 0. 0. 1. 1.] -> [ True] | pred: 0.6908408403396606\n",
      "45: [1. 1. 0. 1. 0. 0. 1. 0.] -> [False] | pred: 0.6916893720626831\n",
      "46: [1. 1. 0. 1. 0. 0. 0. 1.] -> [False] | pred: 0.08330466598272324\n",
      "47: [1. 1. 0. 1. 0. 0. 0. 0.] -> [ True] | pred: 0.691279411315918\n",
      "48: [1. 1. 0. 0. 1. 1. 1. 1.] -> [False] | pred: 0.020223375409841537\n",
      "49: [1. 1. 0. 0. 1. 1. 1. 0.] -> [ True] | pred: 0.6889623999595642\n",
      "50: [1. 1. 0. 0. 1. 1. 0. 1.] -> [ True] | pred: 0.9875146150588989\n",
      "51: [1. 1. 0. 0. 1. 1. 0. 0.] -> [False] | pred: 0.026330633088946342\n",
      "52: [1. 1. 0. 0. 1. 0. 1. 1.] -> [ True] | pred: 0.6903258562088013\n",
      "53: [1. 1. 0. 0. 1. 0. 1. 0.] -> [False] | pred: 0.6916893720626831\n",
      "54: [1. 1. 0. 0. 1. 0. 0. 1.] -> [False] | pred: 0.04585348442196846\n",
      "55: [1. 1. 0. 0. 1. 0. 0. 0.] -> [ True] | pred: 0.6910400390625\n",
      "56: [1. 1. 0. 0. 0. 1. 1. 1.] -> [ True] | pred: 0.6893460750579834\n",
      "57: [1. 1. 0. 0. 0. 1. 1. 0.] -> [False] | pred: 0.691689133644104\n",
      "58: [1. 1. 0. 0. 0. 1. 0. 1.] -> [False] | pred: 0.03460099548101425\n",
      "59: [1. 1. 0. 0. 0. 1. 0. 0.] -> [ True] | pred: 0.6905671954154968\n",
      "60: [1. 1. 0. 0. 0. 0. 1. 1.] -> [False] | pred: 0.6916893720626831\n",
      "61: [1. 1. 0. 0. 0. 0. 1. 0.] -> [ True] | pred: 0.6916896104812622\n",
      "62: [1. 1. 0. 0. 0. 0. 0. 1.] -> [ True] | pred: 0.6911303997039795\n",
      "63: [1. 1. 0. 0. 0. 0. 0. 0.] -> [False] | pred: 0.6916893720626831\n",
      "64: [1. 0. 1. 1. 1. 1. 1. 1.] -> [ True] | pred: 0.9858139157295227\n",
      "65: [1. 0. 1. 1. 1. 1. 1. 0.] -> [False] | pred: 0.015317383222281933\n",
      "66: [1. 0. 1. 1. 1. 1. 0. 1.] -> [False] | pred: 0.2836059033870697\n",
      "67: [1. 0. 1. 1. 1. 1. 0. 0.] -> [ True] | pred: 0.9318113327026367\n",
      "68: [1. 0. 1. 1. 1. 0. 1. 1.] -> [False] | pred: 0.018750114366412163\n",
      "69: [1. 0. 1. 1. 1. 0. 1. 0.] -> [ True] | pred: 0.6891388893127441\n",
      "70: [1. 0. 1. 1. 1. 0. 0. 1.] -> [ True] | pred: 0.9461924433708191\n",
      "71: [1. 0. 1. 1. 1. 0. 0. 0.] -> [False] | pred: 0.027081990614533424\n",
      "72: [1. 0. 1. 1. 0. 1. 1. 1.] -> [False] | pred: 0.01851581037044525\n",
      "73: [1. 0. 1. 1. 0. 1. 1. 0.] -> [ True] | pred: 0.6872706413269043\n",
      "74: [1. 0. 1. 1. 0. 1. 0. 1.] -> [ True] | pred: 0.9614294171333313\n",
      "75: [1. 0. 1. 1. 0. 1. 0. 0.] -> [False] | pred: 0.020472388714551926\n",
      "76: [1. 0. 1. 1. 0. 0. 1. 1.] -> [ True] | pred: 0.6894800066947937\n",
      "77: [1. 0. 1. 1. 0. 0. 1. 0.] -> [False] | pred: 0.691689133644104\n",
      "78: [1. 0. 1. 1. 0. 0. 0. 1.] -> [False] | pred: 0.03083067014813423\n",
      "79: [1. 0. 1. 1. 0. 0. 0. 0.] -> [ True] | pred: 0.6906373500823975\n",
      "80: [1. 0. 1. 0. 1. 1. 1. 1.] -> [False] | pred: 0.014599485322833061\n",
      "81: [1. 0. 1. 0. 1. 1. 1. 0.] -> [ True] | pred: 0.684737503528595\n",
      "82: [1. 0. 1. 0. 1. 1. 0. 1.] -> [ True] | pred: 0.9567669034004211\n",
      "83: [1. 0. 1. 0. 1. 1. 0. 0.] -> [False] | pred: 0.016876911744475365\n",
      "84: [1. 0. 1. 0. 1. 0. 1. 1.] -> [ True] | pred: 0.6882098317146301\n",
      "85: [1. 0. 1. 0. 1. 0. 1. 0.] -> [False] | pred: 0.6916889548301697\n",
      "86: [1. 0. 1. 0. 1. 0. 0. 1.] -> [False] | pred: 0.02249232493340969\n",
      "87: [1. 0. 1. 0. 1. 0. 0. 0.] -> [ True] | pred: 0.6900342702865601\n",
      "88: [1. 0. 1. 0. 0. 1. 1. 1.] -> [ True] | pred: 0.6856613755226135\n",
      "89: [1. 0. 1. 0. 0. 1. 1. 0.] -> [False] | pred: 0.6916883587837219\n",
      "90: [1. 0. 1. 0. 0. 1. 0. 1.] -> [False] | pred: 0.01835530251264572\n",
      "91: [1. 0. 1. 0. 0. 1. 0. 0.] -> [ True] | pred: 0.6888211369514465\n",
      "92: [1. 0. 1. 0. 0. 0. 1. 1.] -> [False] | pred: 0.6916889548301697\n",
      "93: [1. 0. 1. 0. 0. 0. 1. 0.] -> [ True] | pred: 0.6916896104812622\n",
      "94: [1. 0. 1. 0. 0. 0. 0. 1.] -> [ True] | pred: 0.6902541518211365\n",
      "95: [1. 0. 1. 0. 0. 0. 0. 0.] -> [False] | pred: 0.6916893720626831\n",
      "96: [1. 0. 0. 1. 1. 1. 1. 1.] -> [False] | pred: 0.2981593906879425\n",
      "97: [1. 0. 0. 1. 1. 1. 1. 0.] -> [ True] | pred: 0.9884428381919861\n",
      "98: [1. 0. 0. 1. 1. 1. 0. 1.] -> [ True] | pred: 0.26954522728919983\n",
      "99: [1. 0. 0. 1. 1. 1. 0. 0.] -> [False] | pred: 0.34148383140563965\n",
      "100: [1. 0. 0. 1. 1. 0. 1. 1.] -> [ True] | pred: 0.9888971447944641\n",
      "101: [1. 0. 0. 1. 1. 0. 1. 0.] -> [False] | pred: 0.016534456983208656\n",
      "102: [1. 0. 0. 1. 1. 0. 0. 1.] -> [False] | pred: 0.3262019455432892\n",
      "103: [1. 0. 0. 1. 1. 0. 0. 0.] -> [ True] | pred: 0.9661545753479004\n",
      "104: [1. 0. 0. 1. 0. 1. 1. 1.] -> [ True] | pred: 0.9823631048202515\n",
      "105: [1. 0. 0. 1. 0. 1. 1. 0.] -> [False] | pred: 0.018232174217700958\n",
      "106: [1. 0. 0. 1. 0. 1. 0. 1.] -> [False] | pred: 0.2715637981891632\n",
      "107: [1. 0. 0. 1. 0. 1. 0. 0.] -> [ True] | pred: 0.9863162636756897\n",
      "108: [1. 0. 0. 1. 0. 0. 1. 1.] -> [False] | pred: 0.022241076454520226\n",
      "109: [1. 0. 0. 1. 0. 0. 1. 0.] -> [ True] | pred: 0.6883411407470703\n",
      "110: [1. 0. 0. 1. 0. 0. 0. 1.] -> [ True] | pred: 0.9856234788894653\n",
      "111: [1. 0. 0. 1. 0. 0. 0. 0.] -> [False] | pred: 0.023929869756102562\n",
      "112: [1. 0. 0. 0. 1. 1. 1. 1.] -> [ True] | pred: 0.9890249371528625\n",
      "113: [1. 0. 0. 0. 1. 1. 1. 0.] -> [False] | pred: 0.013972754590213299\n",
      "114: [1. 0. 0. 0. 1. 1. 0. 1.] -> [False] | pred: 0.29382312297821045\n",
      "115: [1. 0. 0. 0. 1. 1. 0. 0.] -> [ True] | pred: 0.9729511737823486\n",
      "116: [1. 0. 0. 0. 1. 0. 1. 1.] -> [False] | pred: 0.015589756891131401\n",
      "117: [1. 0. 0. 0. 1. 0. 1. 0.] -> [ True] | pred: 0.6864170432090759\n",
      "118: [1. 0. 0. 0. 1. 0. 0. 1.] -> [ True] | pred: 0.9767617583274841\n",
      "119: [1. 0. 0. 0. 1. 0. 0. 0.] -> [False] | pred: 0.01861712336540222\n",
      "120: [1. 0. 0. 0. 0. 1. 1. 1.] -> [False] | pred: 0.01951800286769867\n",
      "121: [1. 0. 0. 0. 0. 1. 1. 0.] -> [ True] | pred: 0.6825555562973022\n",
      "122: [1. 0. 0. 0. 0. 1. 0. 1.] -> [ True] | pred: 0.978396475315094\n",
      "123: [1. 0. 0. 0. 0. 1. 0. 0.] -> [False] | pred: 0.01654258370399475\n",
      "124: [1. 0. 0. 0. 0. 0. 1. 1.] -> [ True] | pred: 0.6871217489242554\n",
      "125: [1. 0. 0. 0. 0. 0. 1. 0.] -> [False] | pred: 0.6916885375976562\n",
      "126: [1. 0. 0. 0. 0. 0. 0. 1.] -> [False] | pred: 0.021003659814596176\n",
      "127: [1. 0. 0. 0. 0. 0. 0. 0.] -> [ True] | pred: 0.6895145177841187\n",
      "128: [0. 1. 1. 1. 1. 1. 1. 1.] -> [ True] | pred: 0.9873078465461731\n",
      "129: [0. 1. 1. 1. 1. 1. 1. 0.] -> [False] | pred: 0.023442376405000687\n",
      "130: [0. 1. 1. 1. 1. 1. 0. 1.] -> [False] | pred: 0.2739979028701782\n",
      "131: [0. 1. 1. 1. 1. 1. 0. 0.] -> [ True] | pred: 0.9853960871696472\n",
      "132: [0. 1. 1. 1. 1. 0. 1. 1.] -> [False] | pred: 0.03708454221487045\n",
      "133: [0. 1. 1. 1. 1. 0. 1. 0.] -> [ True] | pred: 0.6907620429992676\n",
      "134: [0. 1. 1. 1. 1. 0. 0. 1.] -> [ True] | pred: 0.9857548475265503\n",
      "135: [0. 1. 1. 1. 1. 0. 0. 0.] -> [False] | pred: 0.06777957826852798\n",
      "136: [0. 1. 1. 1. 0. 1. 1. 1.] -> [False] | pred: 0.059987328946590424\n",
      "137: [0. 1. 1. 1. 0. 1. 1. 0.] -> [ True] | pred: 0.6900993585586548\n",
      "138: [0. 1. 1. 1. 0. 1. 0. 1.] -> [ True] | pred: 0.8773429989814758\n",
      "139: [0. 1. 1. 1. 0. 1. 0. 0.] -> [False] | pred: 0.045689456164836884\n",
      "140: [0. 1. 1. 1. 0. 0. 1. 1.] -> [ True] | pred: 0.6909089088439941\n",
      "141: [0. 1. 1. 1. 0. 0. 1. 0.] -> [False] | pred: 0.6916893720626831\n",
      "142: [0. 1. 1. 1. 0. 0. 0. 1.] -> [False] | pred: 0.09426753968000412\n",
      "143: [0. 1. 1. 1. 0. 0. 0. 0.] -> [ True] | pred: 0.691309928894043\n",
      "144: [0. 1. 1. 0. 1. 1. 1. 1.] -> [False] | pred: 0.021527713164687157\n",
      "145: [0. 1. 1. 0. 1. 1. 1. 0.] -> [ True] | pred: 0.6891575455665588\n",
      "146: [0. 1. 1. 0. 1. 1. 0. 1.] -> [ True] | pred: 0.9833298325538635\n",
      "147: [0. 1. 1. 0. 1. 1. 0. 0.] -> [False] | pred: 0.0277978777885437\n",
      "148: [0. 1. 1. 0. 1. 0. 1. 1.] -> [ True] | pred: 0.6904241442680359\n",
      "149: [0. 1. 1. 0. 1. 0. 1. 0.] -> [False] | pred: 0.6916893720626831\n",
      "150: [0. 1. 1. 0. 1. 0. 0. 1.] -> [False] | pred: 0.04957529529929161\n",
      "151: [0. 1. 1. 0. 1. 0. 0. 0.] -> [ True] | pred: 0.6910866498947144\n",
      "152: [0. 1. 1. 0. 0. 1. 1. 1.] -> [ True] | pred: 0.6895225048065186\n",
      "153: [0. 1. 1. 0. 0. 1. 1. 0.] -> [False] | pred: 0.691689133644104\n",
      "154: [0. 1. 1. 0. 0. 1. 0. 1.] -> [False] | pred: 0.038544222712516785\n",
      "155: [0. 1. 1. 0. 0. 1. 0. 0.] -> [ True] | pred: 0.6906484961509705\n",
      "156: [0. 1. 1. 0. 0. 0. 1. 1.] -> [False] | pred: 0.6916893720626831\n",
      "157: [0. 1. 1. 0. 0. 0. 1. 0.] -> [ True] | pred: 0.6916896104812622\n",
      "158: [0. 1. 1. 0. 0. 0. 0. 1.] -> [ True] | pred: 0.691172182559967\n",
      "159: [0. 1. 1. 0. 0. 0. 0. 0.] -> [False] | pred: 0.6916893720626831\n",
      "160: [0. 1. 0. 1. 1. 1. 1. 1.] -> [False] | pred: 0.2785803973674774\n",
      "161: [0. 1. 0. 1. 1. 1. 1. 0.] -> [ True] | pred: 0.9905823469161987\n",
      "162: [0. 1. 0. 1. 1. 1. 0. 1.] -> [ True] | pred: 0.2695402503013611\n",
      "163: [0. 1. 0. 1. 1. 1. 0. 0.] -> [False] | pred: 0.29225224256515503\n",
      "164: [0. 1. 0. 1. 1. 0. 1. 1.] -> [ True] | pred: 0.9904738068580627\n",
      "165: [0. 1. 0. 1. 1. 0. 1. 0.] -> [False] | pred: 0.02913086488842964\n",
      "166: [0. 1. 0. 1. 1. 0. 0. 1.] -> [False] | pred: 0.28742077946662903\n",
      "167: [0. 1. 0. 1. 1. 0. 0. 0.] -> [ True] | pred: 0.9888159036636353\n",
      "168: [0. 1. 0. 1. 0. 1. 1. 1.] -> [ True] | pred: 0.9510373473167419\n",
      "169: [0. 1. 0. 1. 0. 1. 1. 0.] -> [False] | pred: 0.06807486712932587\n",
      "170: [0. 1. 0. 1. 0. 1. 0. 1.] -> [False] | pred: 0.2701808214187622\n",
      "171: [0. 1. 0. 1. 0. 1. 0. 0.] -> [ True] | pred: 0.979375422000885\n",
      "172: [0. 1. 0. 1. 0. 0. 1. 1.] -> [False] | pred: 0.1152476891875267\n",
      "173: [0. 1. 0. 1. 0. 0. 1. 0.] -> [ True] | pred: 0.6905030608177185\n",
      "174: [0. 1. 0. 1. 0. 0. 0. 1.] -> [ True] | pred: 0.9751825332641602\n",
      "175: [0. 1. 0. 1. 0. 0. 0. 0.] -> [False] | pred: 0.0656425803899765\n",
      "176: [0. 1. 0. 0. 1. 1. 1. 1.] -> [ True] | pred: 0.9893690347671509\n",
      "177: [0. 1. 0. 0. 1. 1. 1. 0.] -> [False] | pred: 0.019971558824181557\n",
      "178: [0. 1. 0. 0. 1. 1. 0. 1.] -> [False] | pred: 0.27721714973449707\n",
      "179: [0. 1. 0. 0. 1. 1. 0. 0.] -> [ True] | pred: 0.9888333082199097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180: [0. 1. 0. 0. 1. 0. 1. 1.] -> [False] | pred: 0.026710906997323036\n",
      "181: [0. 1. 0. 0. 1. 0. 1. 0.] -> [ True] | pred: 0.6897717714309692\n",
      "182: [0. 1. 0. 0. 1. 0. 0. 1.] -> [ True] | pred: 0.9889373779296875\n",
      "183: [0. 1. 0. 0. 1. 0. 0. 0.] -> [False] | pred: 0.03461579605937004\n",
      "184: [0. 1. 0. 0. 0. 1. 1. 1.] -> [False] | pred: 0.09879478812217712\n",
      "185: [0. 1. 0. 0. 0. 1. 1. 0.] -> [ True] | pred: 0.6884016990661621\n",
      "186: [0. 1. 0. 0. 0. 1. 0. 1.] -> [ True] | pred: 0.939505934715271\n",
      "187: [0. 1. 0. 0. 0. 1. 0. 0.] -> [False] | pred: 0.03251023590564728\n",
      "188: [0. 1. 0. 0. 0. 0. 1. 1.] -> [ True] | pred: 0.6900753378868103\n",
      "189: [0. 1. 0. 0. 0. 0. 1. 0.] -> [False] | pred: 0.691689133644104\n",
      "190: [0. 1. 0. 0. 0. 0. 0. 1.] -> [False] | pred: 0.05597344785928726\n",
      "191: [0. 1. 0. 0. 0. 0. 0. 0.] -> [ True] | pred: 0.6909046173095703\n",
      "192: [0. 0. 1. 1. 1. 1. 1. 1.] -> [False] | pred: 0.2720678150653839\n",
      "193: [0. 0. 1. 1. 1. 1. 1. 0.] -> [ True] | pred: 0.9864221811294556\n",
      "194: [0. 0. 1. 1. 1. 1. 0. 1.] -> [ True] | pred: 0.2695385813713074\n",
      "195: [0. 0. 1. 1. 1. 1. 0. 0.] -> [False] | pred: 0.275863915681839\n",
      "196: [0. 0. 1. 1. 1. 0. 1. 1.] -> [ True] | pred: 0.9860399961471558\n",
      "197: [0. 0. 1. 1. 1. 0. 1. 0.] -> [False] | pred: 0.017015846446156502\n",
      "198: [0. 0. 1. 1. 1. 0. 0. 1.] -> [False] | pred: 0.2745200991630554\n",
      "199: [0. 0. 1. 1. 1. 0. 0. 0.] -> [ True] | pred: 0.9711084961891174\n",
      "200: [0. 0. 1. 1. 0. 1. 1. 1.] -> [ True] | pred: 0.7651590704917908\n",
      "201: [0. 0. 1. 1. 0. 1. 1. 0.] -> [False] | pred: 0.01975833997130394\n",
      "202: [0. 0. 1. 1. 0. 1. 0. 1.] -> [False] | pred: 0.26971596479415894\n",
      "203: [0. 0. 1. 1. 0. 1. 0. 0.] -> [ True] | pred: 0.9139364957809448\n",
      "204: [0. 0. 1. 1. 0. 0. 1. 1.] -> [False] | pred: 0.024636520072817802\n",
      "205: [0. 0. 1. 1. 0. 0. 1. 0.] -> [ True] | pred: 0.688582181930542\n",
      "206: [0. 0. 1. 1. 0. 0. 0. 1.] -> [ True] | pred: 0.8859439492225647\n",
      "207: [0. 0. 1. 1. 0. 0. 0. 0.] -> [False] | pred: 0.025288023054599762\n",
      "208: [0. 0. 1. 0. 1. 1. 1. 1.] -> [ True] | pred: 0.9806313514709473\n",
      "209: [0. 0. 1. 0. 1. 1. 1. 0.] -> [False] | pred: 0.014232625253498554\n",
      "210: [0. 0. 1. 0. 1. 1. 0. 1.] -> [False] | pred: 0.27167269587516785\n",
      "211: [0. 0. 1. 0. 1. 1. 0. 0.] -> [ True] | pred: 0.9721444249153137\n",
      "212: [0. 0. 1. 0. 1. 0. 1. 1.] -> [False] | pred: 0.01602693647146225\n",
      "213: [0. 0. 1. 0. 1. 0. 1. 0.] -> [ True] | pred: 0.6867930293083191\n",
      "214: [0. 0. 1. 0. 1. 0. 0. 1.] -> [ True] | pred: 0.9736745357513428\n",
      "215: [0. 0. 1. 0. 1. 0. 0. 0.] -> [False] | pred: 0.019212251529097557\n",
      "216: [0. 0. 1. 0. 0. 1. 1. 1.] -> [False] | pred: 0.021693341434001923\n",
      "217: [0. 0. 1. 0. 0. 1. 1. 0.] -> [ True] | pred: 0.6832091808319092\n",
      "218: [0. 0. 1. 0. 0. 1. 0. 1.] -> [ True] | pred: 0.7087385654449463\n",
      "219: [0. 0. 1. 0. 0. 1. 0. 0.] -> [False] | pred: 0.017124254256486893\n",
      "220: [0. 0. 1. 0. 0. 0. 1. 1.] -> [ True] | pred: 0.6874504089355469\n",
      "221: [0. 0. 1. 0. 0. 0. 1. 0.] -> [False] | pred: 0.6916887760162354\n",
      "222: [0. 0. 1. 0. 0. 0. 0. 1.] -> [False] | pred: 0.022125232964754105\n",
      "223: [0. 0. 1. 0. 0. 0. 0. 0.] -> [ True] | pred: 0.6896700859069824\n",
      "224: [0. 0. 0. 1. 1. 1. 1. 1.] -> [ True] | pred: 0.2695392370223999\n",
      "225: [0. 0. 0. 1. 1. 1. 1. 0.] -> [False] | pred: 0.2824012041091919\n",
      "226: [0. 0. 0. 1. 1. 1. 0. 1.] -> [False] | pred: 0.26953792572021484\n",
      "227: [0. 0. 0. 1. 1. 1. 0. 0.] -> [ True] | pred: 0.2695412039756775\n",
      "228: [0. 0. 0. 1. 1. 0. 1. 1.] -> [False] | pred: 0.2796684205532074\n",
      "229: [0. 0. 0. 1. 1. 0. 1. 0.] -> [ True] | pred: 0.9897184371948242\n",
      "230: [0. 0. 0. 1. 1. 0. 0. 1.] -> [ True] | pred: 0.26954054832458496\n",
      "231: [0. 0. 0. 1. 1. 0. 0. 0.] -> [False] | pred: 0.29498523473739624\n",
      "232: [0. 0. 0. 1. 0. 1. 1. 1.] -> [False] | pred: 0.2699028253555298\n",
      "233: [0. 0. 0. 1. 0. 1. 1. 0.] -> [ True] | pred: 0.9664440751075745\n",
      "234: [0. 0. 0. 1. 0. 1. 0. 1.] -> [ True] | pred: 0.2695379853248596\n",
      "235: [0. 0. 0. 1. 0. 1. 0. 0.] -> [False] | pred: 0.2704506814479828\n",
      "236: [0. 0. 0. 1. 0. 0. 1. 1.] -> [ True] | pred: 0.9566846489906311\n",
      "237: [0. 0. 0. 1. 0. 0. 1. 0.] -> [False] | pred: 0.02518344111740589\n",
      "238: [0. 0. 0. 1. 0. 0. 0. 1.] -> [False] | pred: 0.2702571749687195\n",
      "239: [0. 0. 0. 1. 0. 0. 0. 0.] -> [ True] | pred: 0.9798471927642822\n",
      "240: [0. 0. 0. 0. 1. 1. 1. 1.] -> [False] | pred: 0.2738921642303467\n",
      "241: [0. 0. 0. 0. 1. 1. 1. 0.] -> [ True] | pred: 0.9891302585601807\n",
      "242: [0. 0. 0. 0. 1. 1. 0. 1.] -> [ True] | pred: 0.2695390284061432\n",
      "243: [0. 0. 0. 0. 1. 1. 0. 0.] -> [False] | pred: 0.2804539203643799\n",
      "244: [0. 0. 0. 0. 1. 0. 1. 1.] -> [ True] | pred: 0.9889017343521118\n",
      "245: [0. 0. 0. 0. 1. 0. 1. 0.] -> [False] | pred: 0.015188791789114475\n",
      "246: [0. 0. 0. 0. 1. 0. 0. 1.] -> [False] | pred: 0.2781350910663605\n",
      "247: [0. 0. 0. 0. 1. 0. 0. 0.] -> [ True] | pred: 0.9839121699333191\n",
      "248: [0. 0. 0. 0. 0. 1. 1. 1.] -> [ True] | pred: 0.8767010569572449\n",
      "249: [0. 0. 0. 0. 0. 1. 1. 0.] -> [False] | pred: 0.02445921115577221\n",
      "250: [0. 0. 0. 0. 0. 1. 0. 1.] -> [False] | pred: 0.2698468863964081\n",
      "251: [0. 0. 0. 0. 0. 1. 0. 0.] -> [ True] | pred: 0.9579607248306274\n",
      "252: [0. 0. 0. 0. 0. 0. 1. 1.] -> [False] | pred: 0.029525309801101685\n",
      "253: [0. 0. 0. 0. 0. 0. 1. 0.] -> [ True] | pred: 0.6852655410766602\n",
      "254: [0. 0. 0. 0. 0. 0. 0. 1.] -> [ True] | pred: 0.9452943205833435\n",
      "255: [0. 0. 0. 0. 0. 0. 0. 0.] -> [False] | pred: 0.019484978169202805\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = predict(params_max)\n",
    "print(\"Predictions of globally optimised NN:\")\n",
    "for i in range(len(y)):\n",
    "    print(f\"{i}: {x[i]} -> {y[i]} | pred: {predictions[i]}\")\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_true=y[:,0],y_score=predictions, pos_label=1)\n",
    "metric = jnp.abs(1-tpr) + jnp.abs(fpr)\n",
    "idx = plt.argmin(metric)\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate\")\n",
    "plt.scatter(fpr[idx], tpr[idx], label=f'optimal threshold {thresholds[idx]}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of globally optimised NN with optimal threshold: 0.8515625\n"
     ]
    }
   ],
   "source": [
    "optimal_thresh = thresholds[idx]\n",
    "classifications = (predictions > optimal_thresh)\n",
    "accuracy = jnp.mean(classifications == y[:,0])\n",
    "print(f\"accuracy of globally optimised NN with optimal threshold: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions of BFGS optimised NN:\n",
      "0: [1. 1. 1. 1. 1. 1. 1. 1.] -> [False] | pred: 3.931084163966641e-23\n",
      "1: [1. 1. 1. 1. 1. 1. 1. 0.] -> [ True] | pred: 1.0\n",
      "2: [1. 1. 1. 1. 1. 1. 0. 1.] -> [ True] | pred: 1.0\n",
      "3: [1. 1. 1. 1. 1. 1. 0. 0.] -> [False] | pred: 0.30311259627342224\n",
      "4: [1. 1. 1. 1. 1. 0. 1. 1.] -> [ True] | pred: 1.0\n",
      "5: [1. 1. 1. 1. 1. 0. 1. 0.] -> [False] | pred: 0.30311259627342224\n",
      "6: [1. 1. 1. 1. 1. 0. 0. 1.] -> [False] | pred: 0.30311259627342224\n",
      "7: [1. 1. 1. 1. 1. 0. 0. 0.] -> [ True] | pred: 0.30311259627342224\n",
      "8: [1. 1. 1. 1. 0. 1. 1. 1.] -> [ True] | pred: 0.7724140286445618\n",
      "9: [1. 1. 1. 1. 0. 1. 1. 0.] -> [False] | pred: 3.794096258964478e-23\n",
      "10: [1. 1. 1. 1. 0. 1. 0. 1.] -> [False] | pred: 3.794443673307137e-23\n",
      "11: [1. 1. 1. 1. 0. 1. 0. 0.] -> [ True] | pred: 1.0\n",
      "12: [1. 1. 1. 1. 0. 0. 1. 1.] -> [False] | pred: 3.794154319127102e-23\n",
      "13: [1. 1. 1. 1. 0. 0. 1. 0.] -> [ True] | pred: 1.0\n",
      "14: [1. 1. 1. 1. 0. 0. 0. 1.] -> [ True] | pred: 1.0\n",
      "15: [1. 1. 1. 1. 0. 0. 0. 0.] -> [False] | pred: 0.30311259627342224\n",
      "16: [1. 1. 1. 0. 1. 1. 1. 1.] -> [ True] | pred: 1.0\n",
      "17: [1. 1. 1. 0. 1. 1. 1. 0.] -> [False] | pred: 0.30311259627342224\n",
      "18: [1. 1. 1. 0. 1. 1. 0. 1.] -> [False] | pred: 0.30311259627342224\n",
      "19: [1. 1. 1. 0. 1. 1. 0. 0.] -> [ True] | pred: 0.30311259627342224\n",
      "20: [1. 1. 1. 0. 1. 0. 1. 1.] -> [False] | pred: 0.30311259627342224\n",
      "21: [1. 1. 1. 0. 1. 0. 1. 0.] -> [ True] | pred: 0.30311259627342224\n",
      "22: [1. 1. 1. 0. 1. 0. 0. 1.] -> [ True] | pred: 0.30311259627342224\n",
      "23: [1. 1. 1. 0. 1. 0. 0. 0.] -> [False] | pred: 0.30311259627342224\n",
      "24: [1. 1. 1. 0. 0. 1. 1. 1.] -> [False] | pred: 7.80853588379784e-23\n",
      "25: [1. 1. 1. 0. 0. 1. 1. 0.] -> [ True] | pred: 1.0\n",
      "26: [1. 1. 1. 0. 0. 1. 0. 1.] -> [ True] | pred: 1.0\n",
      "27: [1. 1. 1. 0. 0. 1. 0. 0.] -> [False] | pred: 0.30311259627342224\n",
      "28: [1. 1. 1. 0. 0. 0. 1. 1.] -> [ True] | pred: 1.0\n",
      "29: [1. 1. 1. 0. 0. 0. 1. 0.] -> [False] | pred: 0.30311259627342224\n",
      "30: [1. 1. 1. 0. 0. 0. 0. 1.] -> [False] | pred: 0.30311259627342224\n",
      "31: [1. 1. 1. 0. 0. 0. 0. 0.] -> [ True] | pred: 0.30311259627342224\n",
      "32: [1. 1. 0. 1. 1. 1. 1. 1.] -> [ True] | pred: 0.7725594639778137\n",
      "33: [1. 1. 0. 1. 1. 1. 1. 0.] -> [False] | pred: 3.796499760370505e-23\n",
      "34: [1. 1. 0. 1. 1. 1. 0. 1.] -> [False] | pred: 3.856403333158092e-23\n",
      "35: [1. 1. 0. 1. 1. 1. 0. 0.] -> [ True] | pred: 1.0\n",
      "36: [1. 1. 0. 1. 1. 0. 1. 1.] -> [False] | pred: 3.810108557618654e-23\n",
      "37: [1. 1. 0. 1. 1. 0. 1. 0.] -> [ True] | pred: 1.0\n",
      "38: [1. 1. 0. 1. 1. 0. 0. 1.] -> [ True] | pred: 1.0\n",
      "39: [1. 1. 0. 1. 1. 0. 0. 0.] -> [False] | pred: 0.30311259627342224\n",
      "40: [1. 1. 0. 1. 0. 1. 1. 1.] -> [False] | pred: 0.772560179233551\n",
      "41: [1. 1. 0. 1. 0. 1. 1. 0.] -> [ True] | pred: 0.772138237953186\n",
      "42: [1. 1. 0. 1. 0. 1. 0. 1.] -> [ True] | pred: 0.7725433707237244\n",
      "43: [1. 1. 0. 1. 0. 1. 0. 0.] -> [False] | pred: 3.794183033664052e-23\n",
      "44: [1. 1. 0. 1. 0. 0. 1. 1.] -> [ True] | pred: 0.7724958658218384\n",
      "45: [1. 1. 0. 1. 0. 0. 1. 0.] -> [False] | pred: 3.794096258964478e-23\n",
      "46: [1. 1. 0. 1. 0. 0. 0. 1.] -> [False] | pred: 3.794877862349371e-23\n",
      "47: [1. 1. 0. 1. 0. 0. 0. 0.] -> [ True] | pred: 1.0\n",
      "48: [1. 1. 0. 0. 1. 1. 1. 1.] -> [False] | pred: 5.311406994223944e-07\n",
      "49: [1. 1. 0. 0. 1. 1. 1. 0.] -> [ True] | pred: 1.0\n",
      "50: [1. 1. 0. 0. 1. 1. 0. 1.] -> [ True] | pred: 1.0\n",
      "51: [1. 1. 0. 0. 1. 1. 0. 0.] -> [False] | pred: 0.30311259627342224\n",
      "52: [1. 1. 0. 0. 1. 0. 1. 1.] -> [ True] | pred: 1.0\n",
      "53: [1. 1. 0. 0. 1. 0. 1. 0.] -> [False] | pred: 0.30311259627342224\n",
      "54: [1. 1. 0. 0. 1. 0. 0. 1.] -> [False] | pred: 0.30311259627342224\n",
      "55: [1. 1. 0. 0. 1. 0. 0. 0.] -> [ True] | pred: 0.30311259627342224\n",
      "56: [1. 1. 0. 0. 0. 1. 1. 1.] -> [ True] | pred: 0.772560179233551\n",
      "57: [1. 1. 0. 0. 0. 1. 1. 0.] -> [False] | pred: 4.8830284929345273e-23\n",
      "58: [1. 1. 0. 0. 0. 1. 0. 1.] -> [False] | pred: 1.1593670569283037e-20\n",
      "59: [1. 1. 0. 0. 0. 1. 0. 0.] -> [ True] | pred: 1.0\n",
      "60: [1. 1. 0. 0. 0. 0. 1. 1.] -> [False] | pred: 1.8981448670215496e-22\n",
      "61: [1. 1. 0. 0. 0. 0. 1. 0.] -> [ True] | pred: 1.0\n",
      "62: [1. 1. 0. 0. 0. 0. 0. 1.] -> [ True] | pred: 1.0\n",
      "63: [1. 1. 0. 0. 0. 0. 0. 0.] -> [False] | pred: 0.30311259627342224\n",
      "64: [1. 0. 1. 1. 1. 1. 1. 1.] -> [ True] | pred: 1.0\n",
      "65: [1. 0. 1. 1. 1. 1. 1. 0.] -> [False] | pred: 0.30311259627342224\n",
      "66: [1. 0. 1. 1. 1. 1. 0. 1.] -> [False] | pred: 0.30311259627342224\n",
      "67: [1. 0. 1. 1. 1. 1. 0. 0.] -> [ True] | pred: 0.30311259627342224\n",
      "68: [1. 0. 1. 1. 1. 0. 1. 1.] -> [False] | pred: 0.30311259627342224\n",
      "69: [1. 0. 1. 1. 1. 0. 1. 0.] -> [ True] | pred: 0.30311259627342224\n",
      "70: [1. 0. 1. 1. 1. 0. 0. 1.] -> [ True] | pred: 0.30311259627342224\n",
      "71: [1. 0. 1. 1. 1. 0. 0. 0.] -> [False] | pred: 0.30311259627342224\n",
      "72: [1. 0. 1. 1. 0. 1. 1. 1.] -> [False] | pred: 3.7942410938266764e-23\n",
      "73: [1. 0. 1. 1. 0. 1. 1. 0.] -> [ True] | pred: 1.0\n",
      "74: [1. 0. 1. 1. 0. 1. 0. 1.] -> [ True] | pred: 1.0\n",
      "75: [1. 0. 1. 1. 0. 1. 0. 0.] -> [False] | pred: 0.30311259627342224\n",
      "76: [1. 0. 1. 1. 0. 0. 1. 1.] -> [ True] | pred: 1.0\n",
      "77: [1. 0. 1. 1. 0. 0. 1. 0.] -> [False] | pred: 0.30311259627342224\n",
      "78: [1. 0. 1. 1. 0. 0. 0. 1.] -> [False] | pred: 0.30311259627342224\n",
      "79: [1. 0. 1. 1. 0. 0. 0. 0.] -> [ True] | pred: 0.30311259627342224\n",
      "80: [1. 0. 1. 0. 1. 1. 1. 1.] -> [False] | pred: 0.30311259627342224\n",
      "81: [1. 0. 1. 0. 1. 1. 1. 0.] -> [ True] | pred: 0.30311259627342224\n",
      "82: [1. 0. 1. 0. 1. 1. 0. 1.] -> [ True] | pred: 0.30311259627342224\n",
      "83: [1. 0. 1. 0. 1. 1. 0. 0.] -> [False] | pred: 0.30311259627342224\n",
      "84: [1. 0. 1. 0. 1. 0. 1. 1.] -> [ True] | pred: 0.30311259627342224\n",
      "85: [1. 0. 1. 0. 1. 0. 1. 0.] -> [False] | pred: 0.30311259627342224\n",
      "86: [1. 0. 1. 0. 1. 0. 0. 1.] -> [False] | pred: 0.30311259627342224\n",
      "87: [1. 0. 1. 0. 1. 0. 0. 0.] -> [ True] | pred: 0.30311259627342224\n",
      "88: [1. 0. 1. 0. 0. 1. 1. 1.] -> [ True] | pred: 1.0\n",
      "89: [1. 0. 1. 0. 0. 1. 1. 0.] -> [False] | pred: 0.30311259627342224\n",
      "90: [1. 0. 1. 0. 0. 1. 0. 1.] -> [False] | pred: 0.30311259627342224\n",
      "91: [1. 0. 1. 0. 0. 1. 0. 0.] -> [ True] | pred: 0.30311259627342224\n",
      "92: [1. 0. 1. 0. 0. 0. 1. 1.] -> [False] | pred: 0.30311259627342224\n",
      "93: [1. 0. 1. 0. 0. 0. 1. 0.] -> [ True] | pred: 0.30311259627342224\n",
      "94: [1. 0. 1. 0. 0. 0. 0. 1.] -> [ True] | pred: 0.30311259627342224\n",
      "95: [1. 0. 1. 0. 0. 0. 0. 0.] -> [False] | pred: 0.30311259627342224\n",
      "96: [1. 0. 0. 1. 1. 1. 1. 1.] -> [False] | pred: 3.824203608728781e-23\n",
      "97: [1. 0. 0. 1. 1. 1. 1. 0.] -> [ True] | pred: 1.0\n",
      "98: [1. 0. 0. 1. 1. 1. 0. 1.] -> [ True] | pred: 0.999053418636322\n",
      "99: [1. 0. 0. 1. 1. 1. 0. 0.] -> [False] | pred: 0.30311259627342224\n",
      "100: [1. 0. 0. 1. 1. 0. 1. 1.] -> [ True] | pred: 1.0\n",
      "101: [1. 0. 0. 1. 1. 0. 1. 0.] -> [False] | pred: 0.30311259627342224\n",
      "102: [1. 0. 0. 1. 1. 0. 0. 1.] -> [False] | pred: 0.30311259627342224\n",
      "103: [1. 0. 0. 1. 1. 0. 0. 0.] -> [ True] | pred: 0.30311259627342224\n",
      "104: [1. 0. 0. 1. 0. 1. 1. 1.] -> [ True] | pred: 0.7725260257720947\n",
      "105: [1. 0. 0. 1. 0. 1. 1. 0.] -> [False] | pred: 3.794154319127102e-23\n",
      "106: [1. 0. 0. 1. 0. 1. 0. 1.] -> [False] | pred: 3.7956017211160016e-23\n",
      "107: [1. 0. 0. 1. 0. 1. 0. 0.] -> [ True] | pred: 1.0\n",
      "108: [1. 0. 0. 1. 0. 0. 1. 1.] -> [False] | pred: 3.794472703388449e-23\n",
      "109: [1. 0. 0. 1. 0. 0. 1. 0.] -> [ True] | pred: 1.0\n",
      "110: [1. 0. 0. 1. 0. 0. 0. 1.] -> [ True] | pred: 1.0\n",
      "111: [1. 0. 0. 1. 0. 0. 0. 0.] -> [False] | pred: 0.30311259627342224\n",
      "112: [1. 0. 0. 0. 1. 1. 1. 1.] -> [ True] | pred: 1.0\n",
      "113: [1. 0. 0. 0. 1. 1. 1. 0.] -> [False] | pred: 0.30311259627342224\n",
      "114: [1. 0. 0. 0. 1. 1. 0. 1.] -> [False] | pred: 0.30311259627342224\n",
      "115: [1. 0. 0. 0. 1. 1. 0. 0.] -> [ True] | pred: 0.30311259627342224\n",
      "116: [1. 0. 0. 0. 1. 0. 1. 1.] -> [False] | pred: 0.30311259627342224\n",
      "117: [1. 0. 0. 0. 1. 0. 1. 0.] -> [ True] | pred: 0.30311259627342224\n",
      "118: [1. 0. 0. 0. 1. 0. 0. 1.] -> [ True] | pred: 0.30311259627342224\n",
      "119: [1. 0. 0. 0. 1. 0. 0. 0.] -> [False] | pred: 0.30311259627342224\n",
      "120: [1. 0. 0. 0. 0. 1. 1. 1.] -> [False] | pred: 7.162867621697353e-22\n",
      "121: [1. 0. 0. 0. 0. 1. 1. 0.] -> [ True] | pred: 1.0\n",
      "122: [1. 0. 0. 0. 0. 1. 0. 1.] -> [ True] | pred: 1.0\n",
      "123: [1. 0. 0. 0. 0. 1. 0. 0.] -> [False] | pred: 0.30311259627342224\n",
      "124: [1. 0. 0. 0. 0. 0. 1. 1.] -> [ True] | pred: 1.0\n",
      "125: [1. 0. 0. 0. 0. 0. 1. 0.] -> [False] | pred: 0.30311259627342224\n",
      "126: [1. 0. 0. 0. 0. 0. 0. 1.] -> [False] | pred: 0.30311259627342224\n",
      "127: [1. 0. 0. 0. 0. 0. 0. 0.] -> [ True] | pred: 0.30311259627342224\n",
      "128: [0. 1. 1. 1. 1. 1. 1. 1.] -> [ True] | pred: 0.7725567817687988\n",
      "129: [0. 1. 1. 1. 1. 1. 1. 0.] -> [False] | pred: 3.7947333430315343e-23\n",
      "130: [0. 1. 1. 1. 1. 1. 0. 1.] -> [False] | pred: 3.811125872642027e-23\n",
      "131: [0. 1. 1. 1. 1. 1. 0. 0.] -> [ True] | pred: 1.0\n",
      "132: [0. 1. 1. 1. 1. 0. 1. 1.] -> [False] | pred: 3.798469703823023e-23\n",
      "133: [0. 1. 1. 1. 1. 0. 1. 0.] -> [ True] | pred: 1.0\n",
      "134: [0. 1. 1. 1. 1. 0. 0. 1.] -> [ True] | pred: 1.0\n",
      "135: [0. 1. 1. 1. 1. 0. 0. 0.] -> [False] | pred: 0.30311259627342224\n",
      "136: [0. 1. 1. 1. 0. 1. 1. 1.] -> [False] | pred: 0.772560179233551\n",
      "137: [0. 1. 1. 1. 0. 1. 1. 0.] -> [ True] | pred: 0.7710229158401489\n",
      "138: [0. 1. 1. 1. 0. 1. 0. 1.] -> [ True] | pred: 0.7724997997283936\n",
      "139: [0. 1. 1. 1. 0. 1. 0. 0.] -> [False] | pred: 3.794096258964478e-23\n",
      "140: [0. 1. 1. 1. 0. 0. 1. 1.] -> [ True] | pred: 0.7723261117935181\n",
      "141: [0. 1. 1. 1. 0. 0. 1. 0.] -> [False] | pred: 3.7940672288831657e-23\n",
      "142: [0. 1. 1. 1. 0. 0. 0. 1.] -> [False] | pred: 3.7942698083636264e-23\n",
      "143: [0. 1. 1. 1. 0. 0. 0. 0.] -> [ True] | pred: 1.0\n",
      "144: [0. 1. 1. 0. 1. 1. 1. 1.] -> [False] | pred: 4.3799609528464775e-14\n",
      "145: [0. 1. 1. 0. 1. 1. 1. 0.] -> [ True] | pred: 1.0\n",
      "146: [0. 1. 1. 0. 1. 1. 0. 1.] -> [ True] | pred: 1.0\n",
      "147: [0. 1. 1. 0. 1. 1. 0. 0.] -> [False] | pred: 0.30311259627342224\n",
      "148: [0. 1. 1. 0. 1. 0. 1. 1.] -> [ True] | pred: 1.0\n",
      "149: [0. 1. 1. 0. 1. 0. 1. 0.] -> [False] | pred: 0.30311259627342224\n",
      "150: [0. 1. 1. 0. 1. 0. 0. 1.] -> [False] | pred: 0.30311259627342224\n",
      "151: [0. 1. 1. 0. 1. 0. 0. 0.] -> [ True] | pred: 0.30311259627342224\n",
      "152: [0. 1. 1. 0. 0. 1. 1. 1.] -> [ True] | pred: 0.772560179233551\n",
      "153: [0. 1. 1. 0. 0. 1. 1. 0.] -> [False] | pred: 4.06767700742747e-23\n",
      "154: [0. 1. 1. 0. 0. 1. 0. 1.] -> [False] | pred: 2.0930871641778073e-22\n",
      "155: [0. 1. 1. 0. 0. 1. 0. 0.] -> [ True] | pred: 1.0\n",
      "156: [0. 1. 1. 0. 0. 0. 1. 1.] -> [False] | pred: 5.966948304712803e-23\n",
      "157: [0. 1. 1. 0. 0. 0. 1. 0.] -> [ True] | pred: 1.0\n",
      "158: [0. 1. 1. 0. 0. 0. 0. 1.] -> [ True] | pred: 1.0\n",
      "159: [0. 1. 1. 0. 0. 0. 0. 0.] -> [False] | pred: 0.30311259627342224\n",
      "160: [0. 1. 0. 1. 1. 1. 1. 1.] -> [False] | pred: 0.772560179233551\n",
      "161: [0. 1. 0. 1. 1. 1. 1. 0.] -> [ True] | pred: 0.7725514769554138\n",
      "162: [0. 1. 0. 1. 1. 1. 0. 1.] -> [ True] | pred: 0.7725594639778137\n",
      "163: [0. 1. 0. 1. 1. 1. 0. 0.] -> [False] | pred: 3.7999770592407194e-23\n",
      "164: [0. 1. 0. 1. 1. 0. 1. 1.] -> [ True] | pred: 0.7725588083267212\n",
      "165: [0. 1. 0. 1. 1. 0. 1. 0.] -> [False] | pred: 3.7956017211160016e-23\n",
      "166: [0. 1. 0. 1. 1. 0. 0. 1.] -> [False] | pred: 3.832849524250003e-23\n",
      "167: [0. 1. 0. 1. 1. 0. 0. 0.] -> [ True] | pred: 1.0\n",
      "168: [0. 1. 0. 1. 0. 1. 1. 1.] -> [ True] | pred: 0.772560179233551\n",
      "169: [0. 1. 0. 1. 0. 1. 1. 0.] -> [False] | pred: 0.772560179233551\n",
      "170: [0. 1. 0. 1. 0. 1. 0. 1.] -> [False] | pred: 0.772560179233551\n",
      "171: [0. 1. 0. 1. 0. 1. 0. 0.] -> [ True] | pred: 0.7723851799964905\n",
      "172: [0. 1. 0. 1. 0. 0. 1. 1.] -> [False] | pred: 0.772560179233551\n",
      "173: [0. 1. 0. 1. 0. 0. 1. 0.] -> [ True] | pred: 0.7718838453292847\n",
      "174: [0. 1. 0. 1. 0. 0. 0. 1.] -> [ True] | pred: 0.7725333571434021\n",
      "175: [0. 1. 0. 1. 0. 0. 0. 0.] -> [False] | pred: 3.794154319127102e-23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176: [0. 1. 0. 0. 1. 1. 1. 1.] -> [ True] | pred: 0.772560179233551\n",
      "177: [0. 1. 0. 0. 1. 1. 1. 0.] -> [False] | pred: 6.470652394944795e-19\n",
      "178: [0. 1. 0. 0. 1. 1. 0. 1.] -> [False] | pred: 0.0013055158779025078\n",
      "179: [0. 1. 0. 0. 1. 1. 0. 0.] -> [ True] | pred: 1.0\n",
      "180: [0. 1. 0. 0. 1. 0. 1. 1.] -> [False] | pred: 1.8992076800117275e-09\n",
      "181: [0. 1. 0. 0. 1. 0. 1. 0.] -> [ True] | pred: 1.0\n",
      "182: [0. 1. 0. 0. 1. 0. 0. 1.] -> [ True] | pred: 1.0\n",
      "183: [0. 1. 0. 0. 1. 0. 0. 0.] -> [False] | pred: 0.30311259627342224\n",
      "184: [0. 1. 0. 0. 0. 1. 1. 1.] -> [False] | pred: 0.772560179233551\n",
      "185: [0. 1. 0. 0. 0. 1. 1. 0.] -> [ True] | pred: 0.772560179233551\n",
      "186: [0. 1. 0. 0. 0. 1. 0. 1.] -> [ True] | pred: 0.772560179233551\n",
      "187: [0. 1. 0. 0. 0. 1. 0. 0.] -> [False] | pred: 6.944208126762317e-23\n",
      "188: [0. 1. 0. 0. 0. 0. 1. 1.] -> [ True] | pred: 0.772560179233551\n",
      "189: [0. 1. 0. 0. 0. 0. 1. 0.] -> [False] | pred: 4.442419700654194e-23\n",
      "190: [0. 1. 0. 0. 0. 0. 0. 1.] -> [False] | pred: 1.5699666892767514e-21\n",
      "191: [0. 1. 0. 0. 0. 0. 0. 0.] -> [ True] | pred: 1.0\n",
      "192: [0. 0. 1. 1. 1. 1. 1. 1.] -> [False] | pred: 3.8023259714721055e-23\n",
      "193: [0. 0. 1. 1. 1. 1. 1. 0.] -> [ True] | pred: 1.0\n",
      "194: [0. 0. 1. 1. 1. 1. 0. 1.] -> [ True] | pred: 1.0\n",
      "195: [0. 0. 1. 1. 1. 1. 0. 0.] -> [False] | pred: 0.30311259627342224\n",
      "196: [0. 0. 1. 1. 1. 0. 1. 1.] -> [ True] | pred: 1.0\n",
      "197: [0. 0. 1. 1. 1. 0. 1. 0.] -> [False] | pred: 0.30311259627342224\n",
      "198: [0. 0. 1. 1. 1. 0. 0. 1.] -> [False] | pred: 0.30311259627342224\n",
      "199: [0. 0. 1. 1. 1. 0. 0. 0.] -> [ True] | pred: 0.30311259627342224\n",
      "200: [0. 0. 1. 1. 0. 1. 1. 1.] -> [ True] | pred: 0.7724348306655884\n",
      "201: [0. 0. 1. 1. 0. 1. 1. 0.] -> [False] | pred: 3.794096258964478e-23\n",
      "202: [0. 0. 1. 1. 0. 1. 0. 1.] -> [False] | pred: 3.7945014179253993e-23\n",
      "203: [0. 0. 1. 1. 0. 1. 0. 0.] -> [ True] | pred: 1.0\n",
      "204: [0. 0. 1. 1. 0. 0. 1. 1.] -> [False] | pred: 3.794183033664052e-23\n",
      "205: [0. 0. 1. 1. 0. 0. 1. 0.] -> [ True] | pred: 1.0\n",
      "206: [0. 0. 1. 1. 0. 0. 0. 1.] -> [ True] | pred: 1.0\n",
      "207: [0. 0. 1. 1. 0. 0. 0. 0.] -> [False] | pred: 0.30311259627342224\n",
      "208: [0. 0. 1. 0. 1. 1. 1. 1.] -> [ True] | pred: 1.0\n",
      "209: [0. 0. 1. 0. 1. 1. 1. 0.] -> [False] | pred: 0.30311259627342224\n",
      "210: [0. 0. 1. 0. 1. 1. 0. 1.] -> [False] | pred: 0.30311259627342224\n",
      "211: [0. 0. 1. 0. 1. 1. 0. 0.] -> [ True] | pred: 0.30311259627342224\n",
      "212: [0. 0. 1. 0. 1. 0. 1. 1.] -> [False] | pred: 0.30311259627342224\n",
      "213: [0. 0. 1. 0. 1. 0. 1. 0.] -> [ True] | pred: 0.30311259627342224\n",
      "214: [0. 0. 1. 0. 1. 0. 0. 1.] -> [ True] | pred: 0.30311259627342224\n",
      "215: [0. 0. 1. 0. 1. 0. 0. 0.] -> [False] | pred: 0.30311259627342224\n",
      "216: [0. 0. 1. 0. 0. 1. 1. 1.] -> [False] | pred: 8.80573621626517e-23\n",
      "217: [0. 0. 1. 0. 0. 1. 1. 0.] -> [ True] | pred: 1.0\n",
      "218: [0. 0. 1. 0. 0. 1. 0. 1.] -> [ True] | pred: 1.0\n",
      "219: [0. 0. 1. 0. 0. 1. 0. 0.] -> [False] | pred: 0.30311259627342224\n",
      "220: [0. 0. 1. 0. 0. 0. 1. 1.] -> [ True] | pred: 1.0\n",
      "221: [0. 0. 1. 0. 0. 0. 1. 0.] -> [False] | pred: 0.30311259627342224\n",
      "222: [0. 0. 1. 0. 0. 0. 0. 1.] -> [False] | pred: 0.30311259627342224\n",
      "223: [0. 0. 1. 0. 0. 0. 0. 0.] -> [ True] | pred: 0.30311259627342224\n",
      "224: [0. 0. 0. 1. 1. 1. 1. 1.] -> [ True] | pred: 0.7725594639778137\n",
      "225: [0. 0. 0. 1. 1. 1. 1. 0.] -> [False] | pred: 3.796933949412739e-23\n",
      "226: [0. 0. 0. 1. 1. 1. 0. 1.] -> [False] | pred: 3.867069048141042e-23\n",
      "227: [0. 0. 0. 1. 1. 1. 0. 0.] -> [ True] | pred: 1.0\n",
      "228: [0. 0. 0. 1. 1. 0. 1. 1.] -> [False] | pred: 3.8128418028830637e-23\n",
      "229: [0. 0. 0. 1. 1. 0. 1. 0.] -> [ True] | pred: 1.0\n",
      "230: [0. 0. 0. 1. 1. 0. 0. 1.] -> [ True] | pred: 1.0\n",
      "231: [0. 0. 0. 1. 1. 0. 0. 0.] -> [False] | pred: 0.30311259627342224\n",
      "232: [0. 0. 0. 1. 0. 1. 1. 1.] -> [False] | pred: 0.772560179233551\n",
      "233: [0. 0. 0. 1. 0. 1. 1. 0.] -> [ True] | pred: 0.7721993327140808\n",
      "234: [0. 0. 0. 1. 0. 1. 0. 1.] -> [ True] | pred: 0.7725460529327393\n",
      "235: [0. 0. 0. 1. 0. 1. 0. 0.] -> [False] | pred: 3.794212063745364e-23\n",
      "236: [0. 0. 0. 1. 0. 0. 1. 1.] -> [ True] | pred: 0.7725052237510681\n",
      "237: [0. 0. 0. 1. 0. 0. 1. 0.] -> [False] | pred: 3.794096258964478e-23\n",
      "238: [0. 0. 0. 1. 0. 0. 0. 1.] -> [False] | pred: 3.795022381667207e-23\n",
      "239: [0. 0. 0. 1. 0. 0. 0. 0.] -> [ True] | pred: 1.0\n",
      "240: [0. 0. 0. 0. 1. 1. 1. 1.] -> [False] | pred: 2.8130282316851662e-06\n",
      "241: [0. 0. 0. 0. 1. 1. 1. 0.] -> [ True] | pred: 1.0\n",
      "242: [0. 0. 0. 0. 1. 1. 0. 1.] -> [ True] | pred: 1.0\n",
      "243: [0. 0. 0. 0. 1. 1. 0. 0.] -> [False] | pred: 0.30311259627342224\n",
      "244: [0. 0. 0. 0. 1. 0. 1. 1.] -> [ True] | pred: 1.0\n",
      "245: [0. 0. 0. 0. 1. 0. 1. 0.] -> [False] | pred: 0.30311259627342224\n",
      "246: [0. 0. 0. 0. 1. 0. 0. 1.] -> [False] | pred: 0.30311259627342224\n",
      "247: [0. 0. 0. 0. 1. 0. 0. 0.] -> [ True] | pred: 0.30311259627342224\n",
      "248: [0. 0. 0. 0. 0. 1. 1. 1.] -> [ True] | pred: 0.772560179233551\n",
      "249: [0. 0. 0. 0. 0. 1. 1. 0.] -> [False] | pred: 5.0948139801592124e-23\n",
      "250: [0. 0. 0. 0. 0. 1. 0. 1.] -> [False] | pred: 2.7068201154026386e-20\n",
      "251: [0. 0. 0. 0. 0. 1. 0. 0.] -> [ True] | pred: 1.0\n",
      "252: [0. 0. 0. 0. 0. 0. 1. 1.] -> [False] | pred: 2.4685936640874026e-22\n",
      "253: [0. 0. 0. 0. 0. 0. 1. 0.] -> [ True] | pred: 1.0\n",
      "254: [0. 0. 0. 0. 0. 0. 0. 1.] -> [ True] | pred: 1.0\n",
      "255: [0. 0. 0. 0. 0. 0. 0. 0.] -> [False] | pred: 0.30311259627342224\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAArsklEQVR4nO3deXxU9bnH8c9D2HcxxAsECMgSJImALFIUDVhBtG6l4oK41NqqVKXW7dYqcm/VVmvrhnWpolZBcSu3otgKilqRRSIIJIIsEqAEkX2REJ77xzkZQ0jCsMyEZL7v12tenHPmN+c8Z4bMM+f3nPM75u6IiEjiqlHZAYiISOVSIhARSXBKBCIiCU6JQEQkwSkRiIgkuJqVHcCBSk5O9rS0tMoOQ0SkSpkzZ8437t68rOeqXCJIS0tj9uzZlR2GiEiVYmYryntOXUMiIglOiUBEJMEpEYiIJDglAhGRBKdEICKS4GKWCMzsGTMrMLMvynnezOxhM1tiZvPMrEesYhERkfLF8ohgHDC4gufPADqGj6uBx2MYi4iIlCNm1xG4+3QzS6ugyTnA8x6Mgz3DzJqaWQt3XxOrmESOdIVFe5i9fAOzl39LYdGeyg5HjjADuxzD8a2bHvb1VuYFZa2AlSXm88Nl+yQCM7ua4KiBNm3axCU4kXhZv/U73s9bx9S8AqZ/uY4tO3cDYFbJgckRJ6Vx3WqXCKLm7k8CTwL07NlTd9KRKs3dWbB6M9NyC5iaV0DOyo24Q/NGdRiS0YLs9BRO6phMwzpV4s9TqoHK/J+2CmhdYj41XCZS7WzftZuPFn/DtLwCpuWu4z+bdwJwfGoTbhzYiQHpKXRt2ZgaNXQYIPFXmYlgEjDSzCYAfYBNqg9IdfL1+u1MzV3L1Lx1zFi6nl2799CwTk1O7pjMgPQUTu2cQvNGdSo7TJHYJQIzGw+cCiSbWT5wF1ALwN3/AkwGhgBLgO3AFbGKRSQeCov2MGfFBqblFvBebgFLCrYC0D65AZee2JaB6Sn0TGtG7Zq6fEeOLLE8a+ii/TzvwHWx2r5IPHy7bRfv5xUwNbeAD8JCb60ko0+7o7modxsGpKfQLrlBZYcpUiFVo0QOgLuzcE1Y6M0tYG5Y6E1uWIczMv6LAekpnNSxuQq9UqXof6vIfmzftZuPl6xnam4B7+cVsGZTUOjNSm3CDQM7MiA9hYyWTVTolSpLiUCkDCu/3c7U8Ff/J2Ght0HtJE7u2JxRP0zh1M7NSWlUt7LDFDkslAhEgN1hobf4y39xWOhtl9yA4X3aMrBLCr1U6JVqSolAEta323bxwZcFvLcouKJ3c1jo7d2uGcN6tWZAegrtmzes7DBFYk6JQBKGu7NozRam5RXw3qK1exV6B3UtLvQm06hurcoOVSSulAikWtuxq4iPl3zD1LwCpuXuXei9fkBQ6M1spUKvJDYlAql2Vn67nWnhuf3//qpUofe0sNDbWIVekWJKBFLl7S7aw2dfb+S93LVMyy3gy7VBoTft6PoM79OWAekp9Gp3FHVqJlVypCJHJiUCqZI2bNvFB1+u473coNC7aUchNWsEhd4LeqrQK3IglAikSnB3cv+zJXJ659yvN7DHIblhbX543DEMVKFX5KApEcgRa8euIv791TdMzQ0KvavDQm9mqyaMHNCRgSr0ihwWSgRyRMnfsD0yeucnX63nu7DQe1LHZG44rSPZnVNU6BU5zJQIpFIVF3qDLp+1kUJv26Prc3GfYPTO3u2aqdArEkNKBBJ3xYXe4qGbSxd6s9NTaJ/cANNNe0XiQolAYq5koXdabgGfhYXeoxsEhd7iK3obq9ArUimUCCQmduwq4pOl3/Deor0LvRmtGjMyuwMDuhxDlgq9IkcEJQI5bFZt3BH09S9ay7/DQm/92kmc1CGZ6wd2JDs9hWNU6BU54igRyEHbXbSHuSvDQu+iAvLWbgGgTbP6kds09mmvQq/IkU6JQA7Ixu3fF3rfz/u+0NsrrRm/GdKF7PQUjm2uQq9IVaJEIBVyd/LWfl/onbPi+0LvaV2CQu/JnVToFanKlAhkHzsLS17Ru45VG3cA0LVlUOjNTk/h+NSmKvSKVBNKBAJ8X+idllvAv7/6hp2FQaG3X4dkfjmggwq9ItWYEkGCKtrjzP16A++FX/65//m+0HthLxV6RRKJEkECKVno/eDLdWzcHhR6e6YdxX8PSWdA+jEq9IokICWCaszd+XLt1sg4PsWF3mYNajMgPSUo9HZsTpN6KvSKJDIlgmpmZ2ERn3y1PjJuf8lC73UlCr1JKvSKSEiJoBpYXaLQ+3FY6K1XKxi6eeSADmR3TuG/mqjQKyJlUyKogooLvcW/+osLva2b1ePCXm3ITk+hT7tm1K2lQq+I7J8SQRWxaXsh738Z/Or/4Mt1bNheSFINo2fb4kJvCsc2b6hCr4gcMCWCI5S7s7hga2T0zjlfb6Boj9OsQW2yO6cwoIsKvSJyeMQ0EZjZYOAhIAl42t3vK/V8G+A5oGnY5jZ3nxzLmI5kOwuL+GTpeqYu2rvQe1yLxlxzyrFkp6fQrbUKvSJyeMUsEZhZEvAY8EMgH5hlZpPcfWGJZncAr7j742Z2HDAZSItVTEeiNZt2REbvLFno7dchOTzLpzktmtSr7DBFpBqL5RFBb2CJuy8FMLMJwDlAyUTgQONwugmwOobxHBGK9jg5K4NC73uLvi/0ph5Vj2HhbRpPbH+0Cr0iEjexTAStgJUl5vOBPqXajAbeNbNfAg2A08pakZldDVwN0KZNm8MeaKxt2l7IB4vXMS23gPfzCiKF3hPaHsXtZwSF3g4pKvSKSOWo7GLxRcA4d/+jmfUFXjCzDHffU7KRuz8JPAnQs2dPr4Q4D4i7s6RgK++Fp3fOWREUeo+qX4tTOwdX9Pbv2Jwm9VXoFZHKF8tEsApoXWI+NVxW0k+BwQDu/omZ1QWSgYIYxnVI3py7ivun5LF64w5aNq3HzYM6c273VuwsLGLG0u+v6M3fEBR6u7RozC9Oac+A9GNU6BWRI1IsE8EsoKOZtSNIABcCF5dq8zUwEBhnZl2AusC6GMZ0SN6cu4rbX5/PjsIiIBi6+eZXP+epD5eydN02dhQWUbdWDU7qkMw1px5LducUWjZVoVdEjmwxSwTuvtvMRgJTCE4NfcbdF5jZGGC2u08CbgKeMrNRBIXjy939iO36uX9KXiQJFCsscnL/s4VL+gRX9PZVoVdEqpiY1gjCawIml1p2Z4nphUC/WMZwOK0Oz+svrWiPM+acjDhHIyJyeNSo7ACqkvK6eVqp+0dEqjAlggNw86DO1CvV7VOvVhI3D+pcSRGJiBy6yj59tEo5t3srAO6atIBNOwpp0bgut56RHlkuIlIVKREcoHO7t+LbbbsY84+FvHNjf10LICJVnrqGREQSnBKBiEiCUyIQEUlwSgQiIglOiUBEJMEpEYiIJDglAhGRBBdVIjCzemamy2dFRKqh/SYCM/sRkAO8E853M7NJMY5LRETiJJojgtEE9x/eCODuOUC7mEUkIiJxFU0iKHT3TaWWHbH3DBARkQMTzVhDC8zsYiDJzDoC1wP/jm1YIiISL9EcEfwS6Ap8B7wEbAJuiGVQIiISP9EcEZzp7r8BflO8wMx+AkyMWVQiIhI30RwR3B7lMhERqYLKPSIwszOAIUArM3u4xFONgd2xDkxEROKjoq6h1cBs4GxgTonlW4BRsQxKRETip9xE4O6fA5+b2UvuXhjHmEREJI6iKRanmdm9wHFA3eKF7t4+ZlGJiEjcRFMsfhZ4nKAukA08D/wtlkGJiEj8RJMI6rn7e4C5+wp3Hw2cGduwREQkXqLpGvrOzGoAi81sJLAKaBjbsEREJF6iOSK4AahPMLTECcBw4LJYBiUiIvFT4RGBmSUBw9z918BW4Iq4RCUiInFT4RGBuxcBJ8UpFhERqQTR1AjmhjeimQhsK17o7q/HLCoREYmbaGoEdYH1wADgR+HjrGhWbmaDzSzPzJaY2W3ltLnAzBaa2QIzeynawEVE5PDY7xGBux9UXSCsLzwG/BDIB2aZ2SR3X1iiTUeCAez6ufsGM0s5mG3F00eLv+GZj5dRv3YSdWpFdctnEZEjWiy/yXoDS9x9qbvvAiYA55Rq8zPgMXffAODuBTGM55Bs2lHILa9+zvC/fkqtpBqMu6I3dWslVXZYIiKHLJoawcFqBawsMZ8P9CnVphOAmX0MJAGj3f2d0isys6uBqwHatGkTk2ArMmXBf/jtm1+wftsufnHKsdx4WkclARGpNmKZCKLdfkfgVCAVmG5mme6+sWQjd38SeBKgZ8+ecbtf8rot3zF60gLemr+GLi0a89fLepGZ2iRemxcRiYv9JgIzOwa4B2jp7meY2XFAX3f/635eugpoXWI+NVxWUj7waTi66TIz+5IgMcyKdgdiwd15Y+4qxvxjIdu/K+LmQZ25un97aiWpJiAi1U8032zjgClAy3D+S+DGKF43C+hoZu3MrDZwITCpVJs3CY4GMLNkgq6ipVGsO2ZWbdzB5c/O4levfE775AZMvuEkrsvuoCQgItVWNF1Dye7+ipndDuDuu82saH8vCtuNJEgiScAz7r7AzMYAs919Uvjc6Wa2ECgCbnb39Qe9N4dgzx7nb5+u4Pdv5+LA6B8dx6V900iqYZURjohI3ESTCLaZ2dGAA5jZicCmaFbu7pOByaWW3Vli2oFfhY9K89W6rdz22jxmLd/AyR2Tuee8TFo3q1+ZIYmIxE00ieAmgi6dY8Oze5oDQ2MaVZzsLtrDkx8u5c//WkzdmjW4f2gWQ09IxUxHASKSOKK5oGyOmZ0CdAYMyKsOt65csHoTt742jy9WbWZQ12P4n3MySGlcd/8vFBGpZqI5a2gewcVgL7v7V7EPKbZ2FhbxyNTF/OWDpRxVvzaPX9KDMzJbVHZYIiKVJpquoR8Bw4BXzGwP8DLwirt/HdPIYmD28m+55bV5LF23jR/3SOW3Z3Whaf3alR2WiEil2u85keHtKf/g7icAFwNZwLKYR3aYTf9yHT954hO+K9zDc1f25o8XHK8kICJClFcWm1lbgqOCYQSned4Sy6BiYdGazbjDpJH9OLphncoOR0TkiBFNjeBToBbB/Qh+4u6VesHXoapXW2MEiYiUFM0RwQh3z4t5JCIiUinKTQRmNtzd/wacaWZnln7e3R+MaWQiIhIXFR0RNAj/bVTGc3EbAVRERGKr3ETg7k+Ek/9y949LPmdm/WIalYiIxE00Q2o+EuUyERGpgiqqEfQFfgA0N7OSg8I1JhhNVEREqoGKagS1gYZhm5J1gs1Uk0HnRESk4hrBB8AHZjbO3VfEMSYREYmjirqG/uzuNwKPmtk+Zwm5+9mxDExEROKjoq6hF8J/H4hHICIiUjkq6hqaE/77QfEyMzsKaO3u8+IQm4iIxMF+Tx81s/fNrLGZNQM+A54yM11VLCJSTURzHUETd98MnA887+59gNNiG5aIiMRLNImgppm1AC4A/hHjeEREJM6iSQRjgCnAV+4+y8zaA4tjG5aIiMRLNDevn0hwL4Li+aXAj2MZlIiIxE80xeJUM3vDzArCx2tmlhqP4EREJPai6Rp6FpgEtAwf/xcuExGRaiCaRNDc3Z91993hYxzQPMZxiYhInESTCNab2XAzSwofw4H1sQ5MRETiI5pEcCXBqaP/CR9DgStiGZSIiMRPNGcNrQA0wJyISDUVzVlD7c3s/8xsXXjW0N/DawlERKQaiKZr6CXgFaAFwVlDE4HxsQxKRETiJ5pEUN/dXyhx1tDfgLrRrNzMBptZnpktMbPbKmj3YzNzM+sZbeAiInJ4RJMI3jaz28wszczamtktwGQzaxaOSFomM0sCHgPOAI4DLjKz48po1wi4Afj04HZBREQOxX6LxQRnDAH8vNTyCwEHyqsX9AaWhENSYGYTgHOAhaXa/Q/we+DmaAIWEZHDK5qzhtod5LpbAStLzOcDfUo2MLMeBDe6ecvMyk0EZnY1cDVAmzZtDjIcEREpSzRdQzFhZjWAB4Gb9tfW3Z90957u3rN5c13ULCJyOMUyEawCWpeYTw2XFWsEZADvm9ly4ERgkgrGIiLxFctEMAvoaGbtzKw2QU1hUvGT7r7J3ZPdPc3d04AZwNnuPjuGMYmISCnRXFBm4VhDd4bzbcys9/5e5+67gZEEN7VZBLzi7gvMbIyZ6UplEZEjRDRnDY0F9gADCO5WtgV4Dei1vxe6+2Rgcqlld5bT9tQoYhERkcMsmkTQx917mNlcAHffEHb1iIhINRBNjaAwvDjMAcysOcERgoiIVAPRJIKHgTeAFDP7HfARcE9MoxIRkbiJ5oKyF81sDjAQMOBcd18U88hERCQu9psIzKwNsJ3gXsWRZe7+dSwDExGR+IimWPwWQX3ACEYdbQfkAV1jGJeIiMRJNF1DmSXnw/GBro1ZRCIiElcHfGWxu39GqcHjRESk6oqmRvCrErM1gB7A6phFJCIicRVNjaBRiendBDWD12ITjoiIxFuFiSC8kKyRu/86TvGIiEiclVsjMLOa7l4E9ItjPCIiEmcVHRHMJKgH5JjZJGAisK34SXd/PcaxiYhIHERTI6gLrCcYfbT4egIHlAhERKqBihJBSnjG0Bd8nwCKeUyjEhGRuKkoESQBDdk7ARRTIhARqSYqSgRr3H1M3CIREZFKUdGVxWUdCYiISDVTUSIYGLcoRESk0pSbCNz923gGIiIileOAB50TEZHqRYlARCTBKRGIiCQ4JQIRkQSnRCAikuCUCEREEpwSgYhIglMiEBFJcEoEIiIJTolARCTBxTQRmNlgM8szsyVmdlsZz//KzBaa2Twze8/M2sYyHhER2VfMEkF44/vHgDOA44CLzOy4Us3mAj3dPQt4FfhDrOIREZGyxfKIoDewxN2XuvsuYAJwTskG7j7N3beHszOA1BjGIyIiZYhlImgFrCwxnx8uK89PgbfLesLMrjaz2WY2e926dYcxRBEROSKKxWY2HOgJ3F/W8+7+pLv3dPeezZs3j29wIiLVXEW3qjxUq4DWJeZTw2V7MbPTgN8Ap7j7dzGMR0REyhDLI4JZQEcza2dmtYELgUklG5hZd+AJ4Gx3L4hhLCIiUo6YJQJ33w2MBKYAi4BX3H2BmY0xs7PDZvcDDYGJZpZjZpPKWZ2IiMRILLuGcPfJwORSy+4sMX1aLLcvIiL7d0QUi0VEpPIoEYiIJDglAhGRBKdEICKS4JQIREQSnBKBiEiCUyIQEUlwSgQiIglOiUBEJMEpEYiIJDglAhGRBKdEICKS4JQIREQSnBKBiEiCUyIQEUlwSgQiIglOiUBEJMEpEYiIJDglAhGRBKdEICKS4JQIREQSXM3KDkCObIWFheTn57Nz587KDkVEolC3bl1SU1OpVatW1K9RIpAK5efn06hRI9LS0jCzyg5HRCrg7qxfv578/HzatWsX9evUNSQV2rlzJ0cffbSSgEgVYGYcffTRB3wEr0Qg+6UkIFJ1HMzfqxKBiEiCUyKQaufPf/4z27dvj8wPGTKEjRs3HvJ633//fc4666x9lufk5DB58uTI/OjRo3nggQcOeXulpaWl8c0330Tdfty4cYwcObLM5xo2bFjm8nfeeYfOnTvToUMH7rvvvjLbjBo1im7dutGtWzc6depE06ZNAZg2bVpkebdu3ahbty5vvvkmAI8++igdOnTAzPbaB3fn+uuvp0OHDmRlZfHZZ58BwXvat29funbtSlZWFi+//HLkNZdccgmdO3cmIyODK6+8ksLCQgA2bNjAeeedR1ZWFr179+aLL76IvObKK68kJSWFjIyMvfZl9OjRtGrVKhJz8edYWFjIZZddRmZmJl26dOHee+/d63VFRUV07959r/8Py5Yto0+fPnTo0IFhw4axa9cuAKZPn06PHj2oWbMmr7766j7v5+bNm0lNTd3rsxo/fjyZmZlkZWUxePDgyHv2+eef07dvXzIzM/nRj37E5s2by/yMDpQSgRxWb85dRb/7ptLutrfod99U3py7Ku4xlE4EkydPjnxZxULpRBANd2fPnj0xiujgFBUVcd111/H222+zcOFCxo8fz8KFC/dp96c//YmcnBxycnL45S9/yfnnnw9AdnZ2ZPnUqVOpX78+p59+OgD9+vXjX//6F23btt1rXW+//TaLFy9m8eLFPPnkk1xzzTUA1K9fn+eff54FCxbwzjvvcOONN0aS+SWXXEJubi7z589nx44dPP300wDcc889dOvWjXnz5vH8889zww03RLZz+eWX884775S536NGjYrEPWTIEAAmTpzId999x/z585kzZw5PPPEEy5cvj7zmoYceokuXLnut59Zbb2XUqFEsWbKEo446ir/+9a8AtGnThnHjxnHxxReXuf3f/va39O/fPzK/e/dubrjhBqZNm8a8efPIysri0UcfBeCqq67ivvvuY/78+Zx33nncf//9Za7zQCkRyGHz5txV3P76fFZt3IEDqzbu4PbX5x9yMnjwwQfJyMggIyODP//5zwAsX76c9PR0LrnkErp06cLQoUPZvn07Dz/8MKtXryY7O5vs7Gzg+1/Sxa+5/PLL6dSpE5dccgn/+te/6NevHx07dmTmzJkAzJw5k759+9K9e3d+8IMfkJeXV25su3bt4s477+Tll1+mW7dukV+uCxcu5NRTT6V9+/Y8/PDDkZg7d+7MiBEjyMjIYOXKldx///306tWLrKws7rrrLgC2bdvGmWeeyfHHH09GRsZev4YfeeQRevToQWZmJrm5uQB8++23nHvuuWRlZXHiiScyb968feJctmxZ5JfkHXfcUea+zJw5kw4dOtC+fXtq167NhRdeyN///vcKP5vx48dz0UUX7bP81Vdf5YwzzqB+/foAdO/enbS0tH3a/f3vf2fEiBGYGSeeeCIbN25kzZo1dOrUiY4dOwLQsmVLUlJSWLduHRAc4ZkZZkbv3r3Jz8+PvOcDBgwAID09neXLl7N27VoA+vfvT7NmzSrcl5LMjG3btrF792527NhB7dq1ady4MRCcSffWW29x1VVXRdq7O1OnTmXo0KEAXHbZZZGjobS0NLKysqhRY9+v2zlz5rB27dpIwixel7uzbds23J3NmzfTsmVLAL788stI0vjhD3/Ia6+9FvU+VUSJQA6b+6fksaOwaK9lOwqLuH9K+V+k+zNnzhyeffZZPv30U2bMmMFTTz3F3LlzAcjLy+Paa69l0aJFNG7cmLFjx3L99dfTsmVLpk2bxrRp0/ZZ35IlS7jpppvIzc0lNzeXl156iY8++ogHHniAe+65Bwi+RD788EPmzp3LmDFj+O///u9y46tduzZjxoxh2LBh5OTkMGzYMAByc3OZMmUKM2fO5O677450XyxevJhrr72WBQsWkJeXx+LFi5k5cyY5OTnMmTOH6dOn884779CyZUs+//xzvvjiCwYPHhzZXnJyMp999hnXXHNNpPvprrvuonv37sybN4977rmHESNG7BPnDTfcwDXXXMP8+fNp0aJFmfuyatUqWrduHZlPTU1l1aryk/iKFStYtmxZ5Mu3pAkTJpSZIA5mmzNnzmTXrl0ce+yxey0vLCzkhRdeiLw/xx9/PK+//nrkNStWrIgkiYo8+uijZGVlceWVV7JhwwYAhg4dSoMGDWjRogVt2rTh17/+dSSR3HjjjfzhD3/Y64t9/fr1NG3alJo1a5a7H6Xt2bOHm266aZ9uxFq1avH444+TmZlJy5YtWbhwIT/96U8B6Nq1ayQ5T5w4kZUrV+53/6KhRCCHzeqNOw5oeTQ++ugjzjvvPBo0aEDDhg05//zz+fDDDwFo3bo1/fr1A2D48OF89NFH+11fu3btyMzMpEaNGnTt2pWBAwdiZmRmZkYO/Tdt2sRPfvITMjIyGDVqFAsWLDjguM8880zq1KlDcnIyKSkpkV+mbdu25cQTTwTg3Xff5d1336V79+706NGD3NxcFi9eTGZmJv/85z+59dZb+fDDD2nSpElkvcXdMCeccEIk3o8++ohLL70UgAEDBrB+/fp9+o4//vjjyBdzcdtDNWHCBIYOHUpSUtJey9esWcP8+fMZNGjQIW9jzZo1XHrppTz77LP7/KK+9tpr6d+/PyeffDIAt912Gxs3bqRbt2488sgjdO/efZ/YSrvmmmv46quvyMnJoUWLFtx0001AkEiSkpJYvXo1y5Yt449//CNLly7lH//4BykpKZxwwgmHvG9jx45lyJAhpKam7rW8sLCQxx9/nLlz57J69WqysrIiNYpnnnmGsWPHcsIJJ7BlyxZq1659yHFAjC8oM7PBwENAEvC0u99X6vk6wPPACcB6YJi7L49lTBI7LZvWY1UZX/otm9aLyfZKnyYXzWlzderUiUzXqFEjMl+jRg12794NBH222dnZvPHGGyxfvpxTTz31gGMruZ2kpKTIuhs0aBBZ7u7cfvvt/PznP9/n9Z999hmTJ0/mjjvuYODAgdx55517rbfkOqO1v/enVatWe/3CzM/Pp1WrVuW2nzBhAo899tg+y1955RXOO++8qK5srWibmzdv5swzz+R3v/tdJHkWu/vuu1m3bh1PPPFEZFnjxo159tlngeC9bdeuHe3bt69w+8ccc0xk+mc/+1mk+PvSSy8xePBgatWqRUpKCv369WP27NnMnTuXSZMmMXnyZHbu3MnmzZsZPnw4L7zwAhs3bmT37t3UrFlzv+8dwCeffMKHH37I2LFj2bp1K7t27aJhw4b8+Mc/BogcAV1wwQWRwn16ejrvvvsuEHQTvfXWWxVuI1oxOyIwsyTgMeAM4DjgIjM7rlSznwIb3L0D8Cfg97GKR2Lv5kGdqVdr719g9WolcfOgzge9zpNPPpk333yT7du3s23bNt54443IL8Cvv/6aTz75BAj+cE866SQAGjVqxJYtWw56m5s2bYr8EY8bN26/7Q92e4MGDeKZZ55h69atQNBNUlBQwOrVq6lfvz7Dhw/n5ptvjpxJU56TTz6ZF198EQjObEpOTo70Zxfr168fEyZMAIi0La1Xr14sXryYZcuWsWvXLiZMmMDZZ59dZtvc3Fw2bNhA375993muvLpBWc4++2yef/553J0ZM2bQpEkTWrRowa5duzjvvPMYMWJEpN+92NNPP82UKVMYP378XkcJGzdujJyp8/TTT9O/f/993ofS1qxZE5l+4403ImcVtWnThqlTpwJBzWbGjBmkp6dz7733kp+fz/Lly5kwYQIDBgzgb3/7G2ZGdnZ25Kyg5557jnPOOafCbb/44ot8/fXXLF++nAceeIARI0Zw33330apVKxYuXBipifzzn/+MFKYLCgqAoFvpf//3f/nFL35R8RscpVh2DfUGlrj7UnffBUwASr8z5wDPhdOvAgNNVy9VWed2b8W952fSqmk9DGjVtB73np/Jud0r/mVUkR49enD55ZfTu3dv+vTpw1VXXUX37t0B6Ny5M4899hhdunRhw4YNkTNOrr76agYPHhwpFh+oW265hdtvv53u3btH9as7OzubhQsX7lUsjsbpp5/OxRdfHCniDh06lC1btjB//nx69+5Nt27duPvuu8st7hYbPXo0c+bMISsri9tuu43nnntunzYPPfQQjz32GJmZmeX2XdesWZNHH32UQYMG0aVLFy644AK6du0KwJ133smkSZMibSdMmMCFF164z1HG8uXLWblyJaeccspeyx9++GFSU1PJz88nKysrUmgdMmQI7du3p0OHDvzsZz9j7NixQHBUMX36dMaNGxc5tTMnJweAX/ziF6xdu5a+ffvSrVs3xowZA8CiRYvIyMigc+fOvP322zz00EOR7V900UX07duXvLw8UlNTI2f03HLLLZHTNKdNm8af/vQnAK677jq2bt1K165d6dWrF1dccQVZWVkVfg6///3vefDBB+nQoQPr16+P9OvPmjWL1NRUJk6cyM9//vPIe1qeli1bctddd9G/f3+ysrLIycmJ1KnGjx9Pp06dSE9Pp2XLllxxxRUVrita5u6HZUX7rNhsKDDY3a8K5y8F+rj7yBJtvgjb5IfzX4Vtvim1rquBqwHatGlzwooVKw44nncX/Ic3c1bx4AXdqFur4n5D+d6iRYv2OU3uSLB8+XLOOuusvc4VF5FAWX+3ZjbH3XuW1b5KDDrn7k8CTwL07NnzoDLX6V3/i9O7/tdhjUtEpDqIZdfQKqB1ifnUcFmZbcysJtCEoGgsUqG0tDQdDYgcJrFMBLOAjmbWzsxqAxcCk0q1mQRcFk4PBaZ6rPqq5KDpIxGpOg7m7zVmicDddwMjgSnAIuAVd19gZmPMrPhUhL8CR5vZEuBXwG2xikcOTt26dVm/fr2SgUgVUHw/grp16x7Q62JWLI6Vnj17+uzZsys7jIShO5SJVC3l3aGsyheLpfLUqlXrgO50JCJVj4aYEBFJcEoEIiIJTolARCTBVblisZmtAw780uJAMhD9LZ6qB+1zYtA+J4ZD2ee27t68rCeqXCI4FGY2u7yqeXWlfU4M2ufEEKt9VteQiEiCUyIQEUlwiZYInqzsACqB9jkxaJ8TQ0z2OaFqBCIisq9EOyIQEZFSlAhERBJctUwEZjbYzPLMbImZ7TOiqZnVMbOXw+c/NbO0SgjzsIpin39lZgvNbJ6ZvWdmbSsjzsNpf/tcot2PzczNrMqfahjNPpvZBeFnvcDMXop3jIdbFP+325jZNDObG/7/HlIZcR4uZvaMmRWEd3As63kzs4fD92OemfU45I26e7V6AEnAV0B7oDbwOXBcqTbXAn8Jpy8EXq7suOOwz9lA/XD6mkTY57BdI2A6MAPoWdlxx+Fz7gjMBY4K51MqO+447POTwDXh9HHA8sqO+xD3uT/QA/iinOeHAG8DBpwIfHqo26yORwS9gSXuvtTddwETgHNKtTkHKL7D96vAQCt9F+6qZb/77O7T3H17ODuD4I5xVVk0nzPA/wC/B6rDONrR7PPPgMfcfQOAuxfEOcbDLZp9dqBxON0EWB3H+A47d58OfFtBk3OA5z0wA2hqZi0OZZvVMRG0AlaWmM8Pl5XZxoMb6GwCjo5LdLERzT6X9FOCXxRV2X73OTxkbu3ub8UzsBiK5nPuBHQys4/NbIaZDY5bdLERzT6PBoabWT4wGfhlfEKrNAf6975fuh9BgjGz4UBP4JTKjiWWzKwG8CBweSWHEm81CbqHTiU46ptuZpnuvrEyg4qxi4Bx7v5HM+sLvGBmGe6+p7IDqyqq4xHBKqB1ifnUcFmZbcysJsHh5Pq4RBcb0ewzZnYa8BvgbHf/Lk6xxcr+9rkRkAG8b2bLCfpSJ1XxgnE0n3M+MMndC919GfAlQWKoqqLZ558CrwC4+ydAXYLB2aqrqP7eD0R1TASzgI5m1s7MahMUgyeVajMJuCycHgpM9bAKU0Xtd5/NrDvwBEESqOr9xrCffXb3Te6e7O5p7p5GUBc5292r8n1Oo/m//SbB0QBmlkzQVbQ0jjEebtHs89fAQAAz60KQCNbFNcr4mgSMCM8eOhHY5O5rDmWF1a5ryN13m9lIYArBGQfPuPsCMxsDzHb3ScBfCQ4flxAUZS6svIgPXZT7fD/QEJgY1sW/dvezKy3oQxTlPlcrUe7zFOB0M1sIFAE3u3uVPdqNcp9vAp4ys1EEhePLq/IPOzMbT5DMk8O6x11ALQB3/wtBHWQIsATYDlxxyNuswu+XiIgcBtWxa0hERA6AEoGISIJTIhARSXBKBCIiCU6JQEQkwSkRyBHLzIrMLKfEI62CtlvjGFq5zKylmb0aTncrORKmmZ1d0SipMYglzcwujtf2pOrS6aNyxDKzre7e8HC3jRczu5xgxNORMdxGzXC8rLKeOxX4tbufFavtS/WgIwKpMsysYXgvhc/MbL6Z7TPaqJm1MLPp4RHEF2Z2crj8dDP7JHztRDPbJ2mY2ftm9lCJ1/YOlzczszfDsd9nmFlWuPyUEkcrc82sUfgr/IvwKtgxwLDw+WFmdrmZPWpmTcxsRTgeEmbWwMxWmlktMzvWzN4xszlm9qGZpZcR52gze8HMPia4MDItbPtZ+PhB2PQ+4ORw+6PMLMnM7jezWeG+/PwwfTRS1VX22Nt66FHeg+DK2Jzw8QbBlfCNw+eSCa6sLD6q3Rr+exPwm3A6iWDMoWSCexI0CJffCtxZxvbeB54Kp/sTjgcPPALcFU4PAHLC6f8D+oXTDcP40kq87nLg0RLrj8wDfweyw+lhwNPh9HtAx3C6D8HwJ6XjHA3MAeqF8/WBuuF0R4IrbiG4OvUfJV53NXBHOF0HmA20q+zPWY/Kf1S7ISakWtnh7t2KZ8ysFnCPmfUH9hAMvXsM8J8Sr5kFPBO2fdPdc8zsFIIblnwcDq9RG/iknG2Oh2BMeDNrbGZNgZOAH4fLp5rZ0WbWGPgYeNDMXgRed/d8i/62Fi8TJIBpBEOcjA2PUn7A98OAQPCFXZZJ7r4jnK4FPGpm3QiSZ6dyXnM6kGVmQ8P5JgSJY1m0QUv1pEQgVcklQHPgBHcvtGBU0bolG4Rf4P2BM4FxZvYgsAH4p7tfFMU2ShfNyi2iuft9ZvYWwbgvH5vZIKK/Ac4kgqTWDDgBmAo0ADaWTH4V2FZiehSwFjieoLu3vBgM+KW7T4kyRkkQqhFIVdIEKAiTQDawz32XLbgX81p3fwp4muCWfzOAfmbWIWzTwMzK+9U8LGxzEsGojpuADwmSUHEB9ht332xmx7r7fHf/PcGRSOn+/C0EXVP7cPet4WseIui+KXL3zcAyM/tJuC0zs+OjfF/WeDD+/qUEXWJlbX8KcE14tISZdTKzBlGsX6o5HRFIVfIi8H9mNp+gfzu3jDanAjebWSGwFRjh7uvCM3jGm1lxV8sdBGP1l7bTzOYSdLdcGS4bTdDdNI9gtMfiIcxvDBPSHmABwV3fSt4ycBpwm5nlAPeWsa2XgYlhzMUuAR43szvCGCYQ3Ke3ImOB18xsBPAO3x8tzAOKzOxzYBxB0kkDPrOg72kdcO5+1i0JQKePioTM7H2C0y2r8j0LRA6YuoZERBKcjghERBKcjghERBKcEoGISIJTIhARSXBKBCIiCU6JQEQkwf0/BE7BrTefuRMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = predict(params_bfgs_max)\n",
    "print(\"Predictions of BFGS optimised NN:\")\n",
    "for i in range(len(y)):\n",
    "    print(f\"{i}: {x[i]} -> {y[i]} | pred: {predictions[i]}\")\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_true=y[:,0],y_score=predictions, pos_label=1)\n",
    "metric = jnp.abs(1-tpr) + jnp.abs(fpr)\n",
    "idx = plt.argmin(metric)\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate\")\n",
    "plt.scatter(fpr[idx], tpr[idx], label=f'optimal threshold {thresholds[idx]}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of BFGS optimised NN with optimal threshold: 0.8515625\n"
     ]
    }
   ],
   "source": [
    "optimal_thresh = thresholds[idx]\n",
    "classifications = (predictions > optimal_thresh)\n",
    "accuracy = jnp.mean(classifications == y[:,0])\n",
    "print(f\"accuracy of BFGS optimised NN with optimal threshold: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}