{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Ranking from Binary Comparisons\n",
    "\n",
    "In this task there are a set $\\mathcal{O}$ of $N$ objects with some intrinsic value $v : \\mathcal{O} \\to \\mathbb{R}$, and $M$ raters who can determine relative ordering of value between two objects, $r : \\mathcal{O} \\times \\mathcal{O} \\to \\{0, 1\\}$. That is, the raters are not able to measure the value of each object or even the relative value between objects. Rather, they can only say whether one objects value is higher or lower than another. In this example we do not allow raters to assign equal value between objects.\n",
    "\n",
    "Each rater provides $Q$ ratings between $Q$ random unique pairs of objects, and has a chance $p$ of getting it wrong. For rater $i$, and objects $j$ and $k$, define the rating $r_ijk$ as 1 if $v(o_j) > v(o_k)$ and 0 otherwise.\n",
    "\n",
    "The objective of the experimenter is to deduce the relative value of each object on a given scale.\n",
    "\n",
    "In this example, we will let that scale be the 5 star rating system, so that each object in $\\mathcal{O}$ will be assigned on a value in $[1, 5]$ (inclusive).\n",
    "\n",
    "To do this we apply a prior that each object $o_i \\in \\mathcal{O}$ has a value $v(o_i) \\triangleq v_i \\sim \\mathcal{U}[1,5]$, and then form a likelihood which is the sum of violated constraints.\n",
    "\n",
    "\n",
    "$L(v_{i:T}) = \\sum_{ijk} r_{ijk} \\mathbb{1}(v(o_j) < v(o_k))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "from jax import numpy as jnp, random\n",
    "from jaxns import GlobalOptimiser, PriorChain\n",
    "from jaxns import save_results, marginalise_dynamic\n",
    "from jaxns.prior_transforms import UniformPrior\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_constraints(num_options, num_raters, tests_per_rater, p_wrong):\n",
    "    key, true_key = random.split(random.PRNGKey(47573957), 2)\n",
    "    actual_rank = random.uniform(true_key,shape=(num_options,),minval=1., maxval=5.)\n",
    "\n",
    "    pairs = jnp.asarray(list(combinations(range(num_options), 2)))\n",
    "    I = []\n",
    "    J = []\n",
    "    S = []\n",
    "    for rater in range(num_raters):\n",
    "        key, sample_key1, sample_key2, sample_key3 = random.split(key, 4)\n",
    "        choices = random.choice(sample_key1,pairs.shape[0], shape=(tests_per_rater,), replace=False)\n",
    "        I.append(pairs[choices,0])\n",
    "        J.append(pairs[choices,1])\n",
    "        guess = jnp.where(random.uniform(sample_key1) < p_wrong,\n",
    "                        actual_rank[I[-1]] < actual_rank[J[-1]], # wrong\n",
    "        actual_rank[I[-1]] > actual_rank[J[-1]] # right\n",
    "                          )\n",
    "        S.append(guess)\n",
    "\n",
    "    return actual_rank, jnp.concatenate(I),jnp.concatenate(J),jnp.concatenate(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO[2022-04-08 19:12:33,149]: Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: \n",
      "INFO[2022-04-08 19:12:33,150]: Unable to initialize backend 'gpu': NOT_FOUND: Could not find registered platform with name: \"cuda\". Available platform names are: Interpreter Host\n",
      "INFO[2022-04-08 19:12:33,151]: Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
      "WARNING[2022-04-08 19:12:33,151]: No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_options=10\n",
    "num_raters=20\n",
    "tests_per_rater=3\n",
    "p_wrong=0.1\n",
    "\n",
    "actual_rank, I, J, S = get_constraints(num_options, num_raters, tests_per_rater, p_wrong=p_wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def log_likelihood(rank):\n",
    "    order = rank[I] > rank[J]\n",
    "    violations = jnp.sum(order != S)\n",
    "    return -violations\n",
    "\n",
    "with PriorChain() as prior_chain:\n",
    "    UniformPrior('rank', jnp.ones(num_options), 5*jnp.ones(num_options))\n",
    "\n",
    "go = GlobalOptimiser(loglikelihood=log_likelihood,\n",
    "                   prior_chain=prior_chain)\n",
    "\n",
    "results = go(random.PRNGKey(32564),\n",
    "             termination_frac_likelihood_improvement=1e-3,termination_patience=3,\n",
    "             termination_max_num_likelihood_evaluations=10e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "Termination Conditions:\n",
      "On a plateau\n",
      "--------\n",
      "# likelihood evals: 171570\n",
      "# samples: 939\n",
      "# likelihood evals / sample: 182.7\n",
      "--------\n",
      "Maximum logL=-9.0\n",
      "--------\n",
      "rank[#]: max(L) est.\n",
      "rank[0]: 2.13\n",
      "rank[1]: 2.85\n",
      "rank[2]: 3.24\n",
      "rank[3]: 3.77\n",
      "rank[4]: 4.38\n",
      "rank[5]: 1.61\n",
      "rank[6]: 3.76\n",
      "rank[7]: 1.47\n",
      "rank[8]: 4.67\n",
      "rank[9]: 1.47\n",
      "--------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'--------\\nTermination Conditions:\\nOn a plateau\\n--------\\n# likelihood evals: 171570\\n# samples: 939\\n# likelihood evals / sample: 182.7\\n--------\\nMaximum logL=-9.0\\n--------\\nrank[#]: max(L) est.\\nrank[0]: 2.13\\nrank[1]: 2.85\\nrank[2]: 3.24\\nrank[3]: 3.77\\nrank[4]: 4.38\\nrank[5]: 1.61\\nrank[6]: 3.76\\nrank[7]: 1.47\\nrank[8]: 4.67\\nrank[9]: 1.47\\n--------'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "go.summary(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of violations at maximum likelihood estimate: 9.0\n",
      "Number of violations at actual rank: 9\n",
      "Option 0: True rank=2.3695788383483887, L_Max rank=2.126412868499756\n",
      "Option 1: True rank=2.6293601989746094, L_Max rank=2.8469901084899902\n",
      "Option 2: True rank=2.764465808868408, L_Max rank=3.243753433227539\n",
      "Option 3: True rank=3.8243227005004883, L_Max rank=3.767789602279663\n",
      "Option 4: True rank=2.954990863800049, L_Max rank=4.38162088394165\n",
      "Option 5: True rank=2.3232431411743164, L_Max rank=1.6057441234588623\n",
      "Option 6: True rank=4.1021881103515625, L_Max rank=3.7617671489715576\n",
      "Option 7: True rank=1.4255428314208984, L_Max rank=1.4677155017852783\n",
      "Option 8: True rank=4.619992733001709, L_Max rank=4.666685104370117\n",
      "Option 9: True rank=1.1152663230895996, L_Max rank=1.4665040969848633\n"
     ]
    }
   ],
   "source": [
    "# The maximum likelihood estimate has to fewest violation\n",
    "\n",
    "rank_L_max_estimate = results.sample_L_max['rank']\n",
    "log_L_max = results.log_L_max\n",
    "print(f\"Number of violations at maximum likelihood estimate: {-log_L_max}\")\n",
    "\n",
    "# Compare to median of posterior\n",
    "\n",
    "log_L_actual = log_likelihood(actual_rank)\n",
    "print(f\"Number of violations at actual rank: {-log_L_actual}\")\n",
    "\n",
    "for i in range(num_options):\n",
    "    print(f\"Option {i}: True rank={actual_rank[i]}, L_Max rank={rank_L_max_estimate[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True rank = 9, max(L) rank = 9\n",
      "True rank = 7, max(L) rank = 7\n",
      "True rank = 5, max(L) rank = 5\n",
      "True rank = 0, max(L) rank = 0\n",
      "True rank = 1, max(L) rank = 1\n",
      "True rank = 2, max(L) rank = 2\n",
      "True rank = 4, max(L) rank = 6\n",
      "True rank = 3, max(L) rank = 3\n",
      "True rank = 6, max(L) rank = 4\n",
      "True rank = 8, max(L) rank = 8\n"
     ]
    }
   ],
   "source": [
    "# Lets set what the relative order is.\n",
    "ordering_L_max = jnp.argsort(rank_L_max_estimate)\n",
    "ordering_true = jnp.argsort(actual_rank)\n",
    "for i in range(num_options):\n",
    "    print(f\"True rank = {ordering_true[i]}, max(L) rank = {ordering_L_max[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}